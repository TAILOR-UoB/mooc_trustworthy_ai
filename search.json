[
  {
    "objectID": "cha_wahcc/Calibration example.html",
    "href": "cha_wahcc/Calibration example.html",
    "title": "Calibration example",
    "section": "",
    "text": "In this Section, we will demonstrate some tools to visualise how calibrated your models are. We will get some intuition about what correction is applying a calibrator, and some calibration evaluation measures.\n\n\nCode\n%matplotlib inline\n%precision 2\nimport warnings; warnings.simplefilter('ignore')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pycalib\n\nnp.random.seed(42)\n\nplt.rcParams['figure.figsize'] = (5, 4)\nplt.rcParams[\"figure.autolayout\"] = True\nplt.rcParams[\"savefig.bbox\"] = 'tight'\n#plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Set1.colors)\n\nfrom pycalib.metrics import binary_ECE, binary_MCE, conf_ECE, conf_MCE, classwise_ECE, classwise_MCE"
  },
  {
    "objectID": "cha_wahcc/Calibration example.html#visualisation-and-evaluation-tools",
    "href": "cha_wahcc/Calibration example.html#visualisation-and-evaluation-tools",
    "title": "Calibration example",
    "section": "",
    "text": "In this Section, we will demonstrate some tools to visualise how calibrated your models are. We will get some intuition about what correction is applying a calibrator, and some calibration evaluation measures.\n\n\nCode\n%matplotlib inline\n%precision 2\nimport warnings; warnings.simplefilter('ignore')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pycalib\n\nnp.random.seed(42)\n\nplt.rcParams['figure.figsize'] = (5, 4)\nplt.rcParams[\"figure.autolayout\"] = True\nplt.rcParams[\"savefig.bbox\"] = 'tight'\n#plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Set1.colors)\n\nfrom pycalib.metrics import binary_ECE, binary_MCE, conf_ECE, conf_MCE, classwise_ECE, classwise_MCE"
  },
  {
    "objectID": "cha_wahcc/Calibration example.html#binary-dataset",
    "href": "cha_wahcc/Calibration example.html#binary-dataset",
    "title": "Calibration example",
    "section": "Binary dataset",
    "text": "Binary dataset\nWe will start with a simple binary dataset composed by some blobs. You can change the dataset here to any binary one but the rest of the text assumes the first example is being used.\n\n\nCode\nfrom sklearn.datasets import make_blobs\n\ndataset_binary = make_blobs(n_samples=10000, centers=5,\n                            n_features=2,\n                            random_state=42)\n\ndataset_binary[1][:] = dataset_binary[1] &gt; 2\n\n\nEach dataset is just composed by the features x and labels y. The following is a scatterplot of the dataset.\n\n\nCode\nx, y = dataset_binary\nfig, ax = plt.subplots()\nscatter = ax.scatter(x[:, 0], x[:, 1], c=y, edgecolors= \"black\", cmap=\"viridis_r\")\nhandles, labels = scatter.legend_elements()\nlabels = ['Negative', 'Positive']\nlegend = ax.legend(handles, labels,\n                    loc=\"lower right\", title=\"Classes\")\nax.add_artist(legend)\nax.grid()\nplt.tight_layout()"
  },
  {
    "objectID": "cha_wahcc/Calibration example.html#reliability-diagram",
    "href": "cha_wahcc/Calibration example.html#reliability-diagram",
    "title": "Calibration example",
    "section": "Reliability diagram",
    "text": "Reliability diagram\nWe will first train a classifier and see how calibrated it is with a reliability diagram. We will load first the previously generated binary dataset.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nx, y = dataset_binary\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n\n\nNow we will train a binary classifier.\n\n\nCode\nfrom sklearn.naive_bayes import GaussianNB\nclf_bin = GaussianNB()\nclf_bin.fit(x_train, y_train)\n\nprint('Priors {}'.format(clf_bin.class_prior_))\nprint('Absolute additive value to the variances {}'.format(clf_bin.epsilon_))\nprint('Variances {}'.format(clf_bin.sigma_))\nprint('Mean {}'.format(clf_bin.theta_))\n\n\nPriors [0.6 0.4]\nAbsolute additive value to the variances 3.226318807172663e-08\nVariances [[23.57 43.55]\n [30.53  3.54]]\nMean [[-1.63  1.34]\n [-3.33  5.76]]\n\n\nNow, we will check what are the probabilities output by the classifier in the different regions of the feature space.\n\n\nCode\ndelta = 0.25\nx0_grid = np.arange(x[:, 0].min(), x[:, 0].max(), delta)\nx1_grid = np.arange(x[:, 1].min(), x[:, 1].max(), delta)\nX0, X1 = np.meshgrid(x0_grid, x1_grid)\nY = clf_bin.predict_proba(np.vstack((X0.flatten(), X1.flatten())).T)\n\nfig, ax = plt.subplots()\nCS = ax.contour(X0, X1, Y[:, 1].reshape(X0.shape),\n                levels=[.1, .3, .5, .7, .9])\nax.clabel(CS, inline=1, fmt='%1.1f', fontsize=15)\nax.scatter(x_test[:75, 0], x_test[:75, 1], c=y_test[:75], edgecolors= \"black\", cmap=\"viridis_r\")\n#ax.set_title('Predicted probabilities')\n\nax.text(clf_bin.theta_[0][0], clf_bin.theta_[0][1], \"$\\mu_1$\", ha=\"center\", va=\"center\", size=12,\n        bbox=dict(boxstyle=\"circle,pad=0.3\", fc=\"yellow\", ec=\"black\", lw=2))\nax.text(clf_bin.theta_[1][0], clf_bin.theta_[1][1], \"$\\mu_2$\", ha=\"center\", va=\"center\", size=12, color='white',\n        bbox=dict(boxstyle=\"circle,pad=0.3\", fc=\"blue\", ec=\"black\", lw=2))\n\nax.grid()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nWe can also look at the positive scores given by the classifeir to each test sample. In the following figure we just numbered every instance with an increasing number and we show the corresponding output score. Then on the right we show a histogram of all the scores.\n\n\nCode\nn_bins = 10\nbins = np.linspace(0, 1, n_bins+1)\n\np_clf = clf_bin.predict_proba(x_test)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\nax1.scatter(range(75), p_clf[:75,1], c=y_test[:75], s=30, edgecolor='black', cmap=\"viridis_r\")\nax1.set_ylim(0, 1)\nax1.grid()\nax1.set_xlabel('instance index')\nax1.set_ylabel('predicted proba')\nax2.hist([p_clf[y_test == 0, 1],\n          p_clf[y_test == 1, 1]],\n         bins=bins, orientation='horizontal', stacked=True,\n         color=['yellow', 'blue'], edgecolor='black')\nax2.set_ylim(0, 1)\nax2.grid()\nax2.set_xlabel('samples per bin')\nplt.tight_layout()\nfig.savefig('cal_1_vis_hist.pdf')\n\n\n\n\n\n\n\n\n\nWe can clearly see that the model gives lower scores to the negative class. We can also see that most of the negative instances are in the range of scores between 0 and 0.1. And notice that the last bin between 0.9 and 1.0 does not contain any sample.\nNow, in order to plot the reliability diagram we need to compute the mean scores in each of the bins, as well as the true proportion of positives.\nWe start by assigning each of the instances to the corresponding bin. The function digitize from Numpy returns the index of the corresponding bin for each instance.\n\n\nCode\ndigitized = np.digitize(p_clf[:, 1], bins=bins)\nprint('Bins = {}'.format(bins))\nprint(digitized)\n\n\nBins = [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n[1 1 7 ... 9 1 6]\n\n\nNow we can compute the mean score in each bin\n\n\nCode\npred_means = [p_clf[:, 1][digitized == i].mean() for i in range(1, n_bins+1)]\nprint(pred_means)\n\n\n[0.012432487052501908, 0.15203439630231722, 0.24868816147980233, 0.35145643469960997, 0.45029774080111806, 0.5534547383794531, 0.6486686394892508, 0.7595789271035308, 0.8238768795804186, nan]\n\n\nNotice that the last bin does not contain any sample, and thus it is assigned a NaN. Now we compute the true positive rate in each bin (or the average value of zeros and ones).\n\n\nCode\ny_means = [y_test[digitized == i].mean() for i in range(1, n_bins+1)]\nprint(y_means)\n\n\n[0.000777000777000777, 0.03208556149732621, 0.11538461538461539, 0.22282608695652173, 0.39461883408071746, 0.5326633165829145, 0.7228017883755589, 0.9419354838709677, 1.0, nan]\n\n\nWe can now plot a line with the mean scores in the x axis and the true positive rate on the y axis. We will also show a histogram with the positive and negative samples per bin.\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nax1.plot(pred_means, y_means, 'o-')\nax1.plot([0, 1], [0, 1], 'r--')\nax1.set_xlim(0, 1)\nax1.set_xlabel('Mean predicted value')\nax1.set_ylim(0, 1)\nax1.set_ylabel('Fraction of positives')\nax1.grid()\n\nax2.hist([p_clf[y_test == 0, 1],\n          p_clf[y_test == 1, 1]],\n         bins=bins, orientation='vertical', stacked=True,\n         color=['yellow', 'b'], edgecolor='black',\n         label=['Neg.', 'Pos.'])\nax2.legend()\nax2.set_xlim(0, 1)\nax2.set_xlabel('Predicted probabilities')\nax2.set_ylabel('Samples per bin')\nax2.grid()\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_diag_and_hist.pdf')\n\n\n\n\n\n\n\n\n\nWe can see in the previous reliability diagram that the model is\n\nover-confident in the lower range of scores: predicts scores of 0.2 when the true proportion of positives is around 10%.\nunder-confident in the higher range of scores: predicts scores of 0.8 when the true proportion of positives is 100%.\n\nThe red diagonal shows the reliability diagram for a calibrated model which always predicts the true proportion of positives.\nWe can show the correction that should be applied to the scores for the model to be calibrated.\n\n\nCode\nfig, ax1 = plt.subplots(1, 1, figsize=(7, 7))\nax1.plot(pred_means, y_means, 'o-')\nax1.plot([0, 1], [0, 1], 'r--')\n\nfor x, y in zip(pred_means, y_means):\n    ax1.arrow(x, y, y-x, y-y, width=0.005, length_includes_head=True, color='red')\n\nax1.set_xlim(0, 1)\nax1.set_xlabel('Mean predicted value')\nax1.set_ylim(0, 1)\nax1.set_ylabel('Fraction of positives')\nax1.grid()\n\n\n\n\n\n\n\n\n\nLets train a different classifier in order to compare with the previous one\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf_bin_2 = KNeighborsClassifier(n_neighbors=10)\nclf_bin_2.fit(x_train, y_train)\n\n\nKNeighborsClassifier(n_neighbors=10)\n\n\nIn the PyCalib library we can directly call a function to plot the reliability diagram for a list of output scores, in this case we will demonstrate with the previously trained classifiers\n\n\nCode\nfrom pycalib.visualisations import plot_reliability_diagram\n\nscores_list = [clf_bin.predict_proba(x_test),\n               clf_bin_2.predict_proba(x_test)]\n\nfig = plt.figure()\nplot_reliability_diagram(y_test, scores_list, legend=('NaiveBayes', 'KNN (k=10)'), class_names=['Neg.', 'Pos.'],\n                         fig=fig);\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_nb_knn.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure()\nplot_reliability_diagram(y_test, scores_list, legend=('NaiveBayes', 'KNN (k=10)'), class_names=['Neg.', 'Pos.'],\n                         fig=fig, show_counts=True, show_histogram=False);\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_count_nb_knn.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nhelp(plot_reliability_diagram)\n\n\nHelp on function plot_reliability_diagram in module pycalib.visualisations:\n\nplot_reliability_diagram(labels, scores, legend=None, show_histogram=True, bins=10, class_names=None, fig=None, show_counts=False, errorbar_interval=None, interval_method='beta', fmt='s-', show_correction=False, show_gaps=False, sample_proportion=0, hist_per_class=False, color_list=None, show_bars=False, invert_histogram=False, color_gaps='lightcoral', confidence=False)\n    Plots the reliability diagram of the given scores and true labels\n    \n    Parameters\n    ==========\n    labels : array (n_samples, )\n        Labels indicating the true class.\n    scores : matrix (n_samples, n_classes) or list of matrices\n        Output probability scores for one or several methods.\n    legend : list of strings or None\n        Text to use for the legend.\n    show_histogram : boolean\n        If True, it generates an additional figure showing the number of\n        samples in each bin.\n    bins : int or list of floats\n        Number of bins to create in the scores' space, or list of bin\n        boundaries.\n    class_names : list of strings or None\n        Name of each class, if None it will assign integer numbers starting\n        with 1.\n    fig : matplotlib.pyplot.Figure or None\n        Figure to use for the plots, if None a new figure is created.\n    show_counts : boolean\n        If True shows the number of samples of each bin in its corresponding\n        line marker.\n    errorbar_interval : float or None\n        If a float between 0 and 1 is passed, it shows an errorbar\n        corresponding to a confidence interval containing the specified\n        percentile of the data.\n    interval_method : string (default: 'beta')\n        Method to estimate the confidence interval which uses the function\n        proportion_confint from statsmodels.stats.proportion\n    fmt : string (default: 's-')\n        Format of the lines following the matplotlib.pyplot.plot standard.\n    show_correction : boolean\n        If True shows an arrow for each bin indicating the necessary correction\n        to the average scores in order to be perfectly calibrated.\n    show_gaps : boolean\n        If True shows the gap between the average predictions and the true\n        proportion of positive samples.\n    sample_proportion : float in the interval [0, 1] (default 0)\n        If bigger than 0, it shows the labels of the specified proportion of\n        samples.\n    hist_per_class : boolean\n        If True shows one histogram of the bins per class.\n    color_list : list of strings or None\n        List of string colors indicating the color of each method.\n    show_bars : boolean\n        If True shows bars instead of lines.\n    invert_histogram : boolean\n        If True shows the histogram with the zero on top and highest number of\n        bin samples at the bottom.\n    color_gaps : string\n        Color of the gaps (if shown).\n    confidence : boolean\n        If True shows only the confidence reliability diagram.\n    \n    Regurns\n    =======\n    fig : matplotlib.pyplot.figure\n        Figure with the reliability diagram\n\n\n\n\n\nCode\nclf_bin.predict_proba(x_test)\n\n\narray([[1.00e+00, 7.62e-08],\n       [1.00e+00, 1.72e-10],\n       [3.86e-01, 6.14e-01],\n       ...,\n       [1.64e-01, 8.36e-01],\n       [1.00e+00, 3.91e-09],\n       [4.26e-01, 5.74e-01]])\n\n\n\n\nCode\nprint('Naive Bayes bin-ECE = ', binary_ECE(y_test, clf_bin.predict_proba(x_test)[:,1], bins=n_bins))\nprint('KNN bin-ECE = ', binary_ECE(y_test,  clf_bin_2.predict_proba(x_test)[:,1], bins=n_bins))\n\nprint('Naive Bayes bin-MCE = ', binary_MCE(y_test, clf_bin.predict_proba(x_test)[:,1], bins=n_bins))\nprint('KNN bin-MCE = ', binary_MCE(y_test,  clf_bin_2.predict_proba(x_test)[:,1], bins=n_bins))\n\nprint('Naive Bayes conf-ECE = ', conf_ECE(y_test, clf_bin.predict_proba(x_test), bins=n_bins))\nprint('KNN conf-ECE = ', conf_ECE(y_test,  clf_bin_2.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes conf-MCE = ', conf_MCE(y_test, clf_bin.predict_proba(x_test), bins=n_bins))\nprint('KNN conf-MCE = ', conf_MCE(y_test,  clf_bin_2.predict_proba(x_test), bins=n_bins))\n\n\nNaive Bayes bin-ECE =  0.08079967025663184\nKNN bin-ECE =  0.005960000000000007\nNaive Bayes bin-MCE =  0.1823565567674369\nKNN bin-MCE =  0.09999999999999976\nNaive Bayes conf-ECE =  0.07583467873161782\nKNN conf-ECE =  0.00683999999999987\nNaive Bayes conf-MCE =  0.16170908776716286\nKNN conf-MCE =  0.06666666666666787\n\n\nWe can also see a visualisation that emphasizes the gaps between the proportions of positives of each bin and the perfect diagonal.\nIn this case we will show it for the first classifier.\n\n\nCode\nfrom pycalib.visualisations import plot_binary_reliability_diagram_gaps\n\nest_scores = clf_bin.predict_proba(x_test)\n\nfig, ax = plot_binary_reliability_diagram_gaps(y_test, est_scores, n_bins=10)\n_ = ax.set_title('Reliability Daigram gaps for GaussianNb')\n\nfor x, y in zip(pred_means, y_means):\n    ax.arrow(x, y, y-x, y-y, width=0.005, length_includes_head=True, color='red')\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom pycalib.visualisations import plot_binary_reliability_diagram_gaps\n\nest_scores = clf_bin.predict_proba(x_test)\n\nfig = plt.figure()\nplot_reliability_diagram(y_test, est_scores, bins=10, show_bars=True, show_gaps=True,\n                         class_names=['Neg.', 'Pos.'], fig=fig);\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_bar_gaps.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax1 = plt.subplots(1, 1, figsize=(7, 7))\nax1.plot(pred_means, y_means, 'o-')\nax1.plot([0, 1], [0, 1], 'r--')\n\nfor x, y in zip(pred_means, y_means):\n    ax1.arrow(x, y, y-x, y-y, width=0.007, length_includes_head=True, color='red')\n    \n#for x, y in zip(pred_means, y_means):\n#    ax1.arrow(x, y, x-x, x-y, color='red', head_width=None)\n\nax1.set_xlim(0, 1)\nax1.set_xlabel('Mean predicted value')\nax1.set_ylim(0, 1)\nax1.set_ylabel('Fraction of positives')\nax1.grid()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure()\nplot_reliability_diagram(y_test, est_scores, bins=10, show_correction=True, class_names=['Neg.', 'Pos.'],\n                        fig=fig);\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_lin_corr.pdf')\n\n\n\n\n\n\n\n\n\nWe can compute the Expected Calibration Error (ECE)\n\n\nCode\nfrom pycalib.metrics import ECE, MCE\n\nprint('ECE = ', ECE(y_test, est_scores))\nprint('MCE = ', MCE(y_test, est_scores))\n\n\nECE =  0.13782220005497553\nMCE =  0.18665074402724735\n\n\n\nMulticlass\n\n\nCode\nfrom sklearn.datasets import make_classification\n\nnp.random.seed(42)\n\nn_features = 7\ndataset_ternary = make_classification(n_classes=3, n_samples=10000,\n                                    n_clusters_per_class=3,\n                                    n_features=n_features,\n                                    n_informative=5,\n                                    random_state=42)\n\nx, y = dataset_ternary\n\nfig = plt.figure(figsize=(12, 7))\nfor i in range(n_features):\n    for j in range(n_features):\n        ax = fig.add_subplot(n_features, n_features, 1 + i + j*n_features)\n        plt.scatter(x[:100,i], x[:100,j], c=y[:100], marker='.')\nplt.tight_layout()\nplt.savefig('cal_1_vis_data_ternary.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\nclf_ter = GaussianNB()\nclf_ter.fit(x_train, y_train)\nclf_ter_2 = KNeighborsClassifier(n_neighbors=10)\nclf_ter_2.fit(x_train, y_train)\n\nscores_list = [clf_ter.predict_proba(x_test),\n               clf_ter_2.predict_proba(x_test)]\n\nfig = plt.figure(figsize=(8, 2.3))\n_ = plot_reliability_diagram(y_test, scores_list, legend=('NaiveBayes', 'KNN (k=10)'), bins=n_bins,\n                            fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_nb_knn_ternary.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure()\n_ = plot_reliability_diagram(y_test, scores_list, legend=('NaiveBayes', 'KNN (k=10)'), confidence=True, bins=bins,\n                            fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_nb_knn_conf_ternary.pdf',\n            bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nIn the multiclass case we can focus on the most confident score per sample, and plot only the proportion of correct predictions in each bin.\nThe following plots only show the results for the first classifier.\n\n\nCode\nfrom pycalib.visualisations import plot_multiclass_reliability_diagram_gaps\n\n#x, y = dataset_ternary\n#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n#clf = GaussianNB()\n#clf.fit(x_train, y_train)\n\nprobas = clf_ter.predict_proba(x_test)\n_ = plot_multiclass_reliability_diagram_gaps(y_test, probas, per_class=False, n_bins=n_bins)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure()\n_ = plot_reliability_diagram(y_test, probas, show_bars=True, show_gaps=True, confidence=True, bins=n_bins, fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_conf_ternary.pdf')\n\n\n\n\n\n\n\n\n\nWe can compute the confidence-ECE which only computes expected gap size in the previous plot\n\n    \\mathsf{confidence{-}ECE}  = \\sum_{i=1}^M \\frac{|B_{i}|}{N} |\\bar{y}(B_{i}) - \\bar{p}(B_{i})|\n\n\n\nCode\nfrom pycalib.metrics import conf_ECE, conf_MCE\n\nprint('Naive Bayes conf-ECE = ', conf_ECE(y_test, probas, bins=n_bins))\nprint('KNN conf-ECE = ', conf_ECE(y_test, clf_ter_2.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes conf-ECE = ', conf_MCE(y_test, probas, bins=n_bins))\nprint('KNN conf-ECE = ', conf_MCE(y_test, clf_ter_2.predict_proba(x_test), bins=n_bins))\n\n\nNaive Bayes conf-ECE =  0.10073120102612733\nKNN conf-ECE =  0.033760000000003856\nNaive Bayes conf-ECE =  0.2332292123860582\nKNN conf-ECE =  0.0428745432399551\n\n\nor the maximum gap with the confidence-MCE:\n\n\\mathsf{confidence{-}MCE} = \\max_{i \\in \\{1, \\ldots, M\\}} |\\bar{y}(B_i) - \\bar{p}(B_i)|\n\n\n\nCode\nfrom pycalib.metrics import conf_MCE\n\nprint('conf-MCE = ', conf_MCE(y_test, probas, bins=n_bins))\n\n\nconf-MCE =  0.2332292123860582\n\n\nInstead of only showing the most confident prediction, we can show the scores for each class\n\n\nCode\nfig = plt.figure(figsize=(14, 5))\n_ = plot_multiclass_reliability_diagram_gaps(y_test, probas, per_class=True, fig=fig, n_bins=n_bins)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(8, 2.3))\n_ = plot_reliability_diagram(y_test, probas, show_bars=True, show_gaps=True, bins=n_bins,\n                             fig=fig)\nplt.tight_layout()\n#plt.margins(x=0)\nplt.savefig('cal_1_vis_rel_dia_ternary.pdf')\n\n\n\n\n\n\n\n\n\nWith the previous figure we can get an intuition about how the classwise-ECE is computed, as the expected gap size across all the classes\n\n\\mathsf{classwise{-}ECE}  = \\frac{1}{K}\\sum_{j=1}^K \\sum_{i=1}^M \\frac{|B_{i,j}|}{N} |\\bar{y}_j(B_{i,j}) - \\bar{p}_j(B_{i,j})|,\n\n\n\nCode\nfrom pycalib.metrics import classwise_ECE\n\nprint('Naive Bayes classwise-ECE = ', classwise_ECE(y_test, probas, bins=n_bins))\nprint('KNN classwise-ECE = ', classwise_ECE(y_test, clf_ter_2.predict_proba(x_test), bins=n_bins))\n\n\nNaive Bayes classwise-ECE =  0.07306931580191046\nKNN classwise-ECE =  0.025426666666666653\n\n\nthe classwise-MCE needs to be added, as well as the statistical tests."
  },
  {
    "objectID": "cha_wahcc/Calibration example.html#calibration-maps",
    "href": "cha_wahcc/Calibration example.html#calibration-maps",
    "title": "Calibration example",
    "section": "Calibration maps",
    "text": "Calibration maps\nCalibration maps show the transformation that the calibrator performs to the classifier’s scores. This is a map function from C to C where C is the number of classes. In the binary case it is comonly shown for the positive class\n\nBinary\nWe will load again the binary dataset that we created earlier\n\n\nCode\nx, y = dataset_binary\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n\n\nNow we will need a calibrator in order to show the mapping function. We will train here a classifier with a calibrator on top.\n\n\nCode\nfrom pycalib.models import CalibratedModel, IsotonicCalibration\n\ncal_bin = CalibratedModel(clf_bin, IsotonicCalibration())\ncal_bin.fit(x_train, y_train)\n\n\nCalibratedModel(base_estimator=GaussianNB(), calibrator=IsotonicCalibration())\n\n\nIn the binary case the calibration map corresponds to a 1 to 1 function that can be visualised in two dimensions with the scores of the classifier in the x axis, and the corresponding probabilities from the calibrator in the y axis.\n\n\nCode\nscores_linspace = np.linspace(0, 1, 50)\nscores_linspace = np.vstack((1-scores_linspace, scores_linspace)).T\nfig = plt.figure()\nax = fig.add_subplot(111)\ncalibrated = cal_bin.calibrator.predict_proba(scores_linspace)\nax.plot(scores_linspace[:, 1], calibrated[:, 1], label='Isotonic')\nax.legend()\nax.set_xlabel('Scores')\nax.set_ylabel('Calibrated scores')\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nax.grid()\nplt.tight_layout()\nplt.savefig('cal_1_vis_cal_map_binary.pdf')\n\n\n\n\n\n\n\n\n\nNotice the similarity to the reliability diagram of the classifier, as the calibrator is trying to correct the misscalibration by fitting the scores as best as possible.\n\n\nCode\nfig = plt.figure()\n_ = plot_reliability_diagram(y_test, [cal_bin.base_estimator.predict_proba(x_test),\n                                      cal_bin.predict_proba(x_test)],\n                             legend=('NaiveBayes', ' + Isotonic'), class_names=['Neg.', 'Pos.'],\n                             bins=n_bins, fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_cal_map_nb_binary.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\naux_scores = cal_bin.predict_proba(x_test)\naux_prediction = np.argmax(aux_scores, axis=1)\naccuracy = np.mean(aux_prediction == y_test)\nprint('Accuracy ', accuracy)\naux_confidence = np.max(aux_scores, axis=1)\nidx_correct = aux_prediction == y_test\nidx_incorrect = aux_prediction != y_test\nplt.hist([aux_confidence[idx_correct], aux_confidence[idx_incorrect]])\n#plt.hist(aux_confidence)\naux_bin = [0.9, 1.0]\naux_indices = (aux_confidence &gt; aux_bin[0]) & (aux_confidence &lt; aux_bin[1])\naux_correct = np.sum(aux_prediction[aux_indices] == y_test[aux_indices])\naux_incorrect = np.sum(aux_prediction[aux_indices] != y_test[aux_indices])\nprint('Correct = ', aux_correct)\nprint('Incorrect = ', aux_incorrect)\nprint('Accuracy = ', aux_correct/(aux_correct + aux_incorrect))\n\n\nAccuracy  0.843\nCorrect =  486\nIncorrect =  19\nAccuracy =  0.9623762376237623\n\n\n\n\n\n\n\n\n\n\n\nCode\ncal_bin.predict_proba(x_test).shape\ny_test.shape\n\n\n(5000,)\n\n\n\n\nCode\n_ = plot_reliability_diagram(y_test, [cal_bin.base_estimator.predict_proba(x_test),\n                                      cal_bin.predict_proba(x_test)],\n                             legend=('NaiveBayes', ' + Isotonic'), class_names=['Neg.', 'Pos.'],\n                             bins=n_bins, confidence=True)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_dia_cal_map_nb_conf_binary.pdf')\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint('Naive Bayes bin-ECE = ', binary_ECE(y_test, cal_bin.base_estimator.predict_proba(x_test)[:,1], bins=n_bins))\nprint('+ Isotonic bin-ECE = ', binary_ECE(y_test,  cal_bin.predict_proba(x_test)[:,1], bins=n_bins))\n\nprint('Naive Bayes bin-MCE = ', binary_MCE(y_test, cal_bin.base_estimator.predict_proba(x_test)[:,1], bins=n_bins))\nprint('+ Isotonic bin-MCE = ', binary_MCE(y_test,  cal_bin.predict_proba(x_test)[:,1], bins=n_bins))\n\nprint('Naive Bayes conf-ECE = ', conf_ECE(y_test, cal_bin.base_estimator.predict_proba(x_test), bins=n_bins))\nprint('+ Isotonic conf-ECE = ', conf_ECE(y_test,  cal_bin.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes conf-MCE = ', conf_MCE(y_test, cal_bin.base_estimator.predict_proba(x_test), bins=n_bins))\nprint('+ Isotonic conf-MCE = ', conf_MCE(y_test,  cal_bin.predict_proba(x_test), bins=n_bins))\n\n\nNaive Bayes bin-ECE =  0.079496190619477\n+ Isotonic bin-ECE =  0.01156162517476712\nNaive Bayes bin-MCE =  0.1753553512921694\n+ Isotonic bin-MCE =  0.19255158274553785\nNaive Bayes conf-ECE =  0.07949619061947717\n+ Isotonic conf-ECE =  0.009154236420034963\nNaive Bayes conf-MCE =  0.1491809427878793\n+ Isotonic conf-MCE =  0.5\n\n\n\n\nTernary Calibration maps\nIt is possible to visualise a calibration map for ternary classification problems. We will start by loading the ternary dataset, and training a classifier and a calibrator on top.\n\n\nCode\nfrom dirichletcal import FullDirichletCalibrator\nfrom pycalib.metrics import classwise_MCE\n\nx, y = dataset_ternary\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\ncal_ter = CalibratedModel(clf_ter, FullDirichletCalibrator())\ncal_ter.fit(x_train, y_train)\n\nprint('Naive Bayes confidence-ECE = ', conf_ECE(y_test, clf_ter.predict_proba(x_test), bins=n_bins))\nprint('+ Dirichlet confidence-ECE = ', conf_ECE(y_test, cal_ter.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes confidence-MCE = ', conf_MCE(y_test, clf_ter.predict_proba(x_test), bins=n_bins))\nprint('+ Dirichlet confidence-MCE = ', conf_MCE(y_test, cal_ter.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes classwise-ECE = ', classwise_ECE(y_test, clf_ter.predict_proba(x_test), bins=n_bins))\nprint('+ Dirichlet classwise-ECE = ', classwise_ECE(y_test, cal_ter.predict_proba(x_test), bins=n_bins))\n\nprint('Naive Bayes classwise-MCE = ', classwise_MCE(y_test, clf_ter.predict_proba(x_test), bins=n_bins))\nprint('+ Dirichlet classwise-MCE = ', classwise_MCE(y_test, cal_ter.predict_proba(x_test), bins=n_bins))\n\n\nModuleNotFoundError: No module named 'dirichletcal'\n\n\nWith 3 classes the calibration map corresponds to a function from 3 to 3 dimensions. As this is difficult to visualise we propose several approaches that show part of the map.\nWe will first obtain a 2D mesh in the simplex, trying to cover most of the space\n\n\nCode\nfrom pycalib.visualisations.barycentric import get_mesh_bc\nP_bc_grid = get_mesh_bc(subdiv=4)\n\n\nNow we will obtain the predicted probabilities output by our calibrator for each of the points in the mesh.\n\n\nCode\ncalibrated = cal_ter.calibrator.predict_proba(P_bc_grid)\n\n\nOne way is to consider that the input and output scores live in a 2D simplex. This means that we can visualise the mapping from an input and output scores as a 2D coordinates. Then we will show both coordinates as the begining and end of an arrow.\n\n\nCode\nfrom pycalib.visualisations.barycentric import draw_calibration_map\n\nfig = plt.figure(figsize=(3, 3))\n_ = draw_calibration_map(P_bc_grid, calibrated,  subdiv=4, fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_cal_map_ternary.pdf')\n\n\nWe can also paint the arrows in three different colors, one per class. We will paint them indicating the most confident class at the tip of the arrow (output of the calibrator).\n\n\nCode\ncolor_list = ['Red', 'Orange', 'Blue']\ncolor = [color_list[i] for i in calibrated.argmax(axis=1).astype(int)]\n\nfig = plt.figure(figsize=(3, 3))\n_ = draw_calibration_map(P_bc_grid, calibrated, color=color,  subdiv=4, fig=fig)\nplt.tight_layout()\nplt.savefig('cal_1_vis_cal_map_ternary_color.pdf')\n\n\nAnother way is to show 3 calibration maps from 3 to 1 dimension. Each calibration map shows the full input score to the calibrator, and only one of the classes’ output. The input then will be shown as the position in a 2D simplex, while the output is shown as a heatmap.\n\n\nCode\nfrom pycalib.visualisations.ternary import draw_func_contours, plot_converging_lines_pvalues, get_converging_lines\n\ncmap_list = ['Reds', 'Oranges', 'Blues']\n\nfig = plt.figure(figsize=(8, 3))\nfor i, c in enumerate(['C1', 'C2', 'C3']):\n    ax = fig.add_subplot(1, 3, i+1)\n    ax.set_title('$C_{}$'.format(i+1), loc='left')\n\n    function = lambda x: cal_ter.calibrator.predict_proba(x.reshape(1,-1))[0][i]\n    draw_func_contours(function, labels=['$C_1$', '$C_2$', '$C_3$'], fig=fig, ax=ax, cmap=cmap_list[i])\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_diag_ternary.pdf')\n\n\nNotice that we can cut arbitrary sections on the previous calibration maps and obtain cross-sections that can be visualised as binary calibration maps.\n\n\nCode\nfrom pycalib.visualisations.ternary import plot_converging_lines_pvalues, get_converging_lines\n\nfig = plt.figure(figsize=(8, 5))\nfor i, c in enumerate(['C1', 'C2', 'C3']):\n    ax = fig.add_subplot(2, 3, i+1)\n\n    ax.set_title('$C_{}$'.format(i+1), loc='left')\n\n    function = lambda x: cal_ter.calibrator.predict_proba(x.reshape(1,-1))[0][i]\n    draw_func_contours(function, labels=['$C_1$', '$C_2$', '$C_3$'], fig=fig, ax=ax, cmap=cmap_list[i],\n                       nlevels=100, subdiv=4, draw_lines=5, class_index=i)\n\n    ax2 = fig.add_subplot(2, 3, 3+i+1)\n    lines = get_converging_lines(num_lines=5, mesh_precision=20, class_index=i)\n    plot_converging_lines_pvalues(function, lines, i, ax2)\nplt.tight_layout()\nplt.savefig('cal_1_vis_rel_diag_ternary_lines.pdf')\n\n\nIn the following figure we show side by side the previous calibration maps\n\n\nCode\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 4, 1)\nfig = draw_calibration_map(P_bc_grid, calibrated, color=color, subdiv=5, fig=fig, ax=ax, alpha=0.9)\n\nfor i, c in enumerate(['C1', 'C2', 'C3']):\n    ax = fig.add_subplot(1, 4, i+2)\n    ax.set_title('$C_{}$'.format(i+1), loc='left')\n\n    function = lambda x: cal_ter.calibrator.predict_proba(x.reshape(1,-1))[0][i]\n    draw_func_contours(function, labels=['$C_1$', '$C_2$', '$C_3$'], fig=fig, ax=ax, cmap=cmap_list[i])"
  },
  {
    "objectID": "explainable-ai-systems.html",
    "href": "explainable-ai-systems.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "This is the content of the explainable AI systems section"
  },
  {
    "objectID": "explainable-ai-systems.html#explainable-ai-systems",
    "href": "explainable-ai-systems.html#explainable-ai-systems",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "This is the content of the explainable AI systems section"
  },
  {
    "objectID": "cha_odm/odm.html",
    "href": "cha_odm/odm.html",
    "title": "Introduction to Optimal Decision Making",
    "section": "",
    "text": "This is an introductory course on the topic of Optimal Decision Making for multiclass classification. In Section 1, we give some motivation insights about the importance of estimating missclassification costs when making automatic decisions, then we define a common setting to calculate expected costs and how to make an optimal decision based on those expected costs in the binary (Section 2.1) and multiclass (Section 2.6) case. We provide interactive examples that demonstrate how the optimal decision change under different operational conditions. Finally in Section 2.8, we demonstrate examples in which abstaining on making a prediction and delegating the decision may be optimal solution.",
    "crumbs": [
      "Introduction to Optimal Decision Making"
    ]
  },
  {
    "objectID": "cha_odm/odm.html#sec-motivation",
    "href": "cha_odm/odm.html#sec-motivation",
    "title": "Introduction to Optimal Decision Making",
    "section": "Motivation",
    "text": "Motivation\n\n\n\nOptimal Decision Making. Why and how?\nOne of the most common objectives of multiclass classification is to train a machine learning model that is able to predict the most probable class given a new instance. However, in certain applications in which the missclassification costs are of main importance, the most probable class may not be the one that provides the highest expected utilities. The objective of optimal decision making is to predict the class that maximises the expected utilities, and minimises the expected misclassification costs. Some typical scenarios in which this is important are medical diagnosis, self-driving cars, extreme weather prediction and finances.\n\nKey points\n\n\nThe objective is to classify a new instance into one of the possible classes in an optimal manner.\nThis may be important in critical applications: e.g. medical diagnosis (Begoli, Bhattacharya, and Kusnezov 2019; Yang, Steinfeld, and Zimmerman 2019), self-driving cars (Qayyum et al. 2020; Mullins et al. 2018), extreme weather prediction, finances (Nti, Adekoya, and Weyori 2020). \nIt is necessary to know what are the consequences of making each prediction (costs or gains).\nOne way to make optimal decisions is with cost-sensitive classification.\nCan we make optimal decisions with any type of classifier?\n\n\n\n\nOptimal decisions with different types of model\n\nClass estimation: Outputs a class prediction.\nClass estimation with option of abstaining: Outputs a class prediction or abstains (Coenen, Abdullah, and Guns 2020; Mozannar and Sontag 2020)\nRankings estimation: Outputs a ranked list of possible classes (Brinker and Hüllermeier 2020).\nScore surrogates: Outputs a continuous score which is commonly a surrogate for classification (e.g. Support Vector Machines).\nProbability estimation: Outputs class posterior probability estimates (e.g. Logistic Regression, naive Bayes, Artificial Neural Networks), or provides class counts which can be interpreted as proportions (e.g. decision trees, random forests, k-nearest neightbour) (Zadrozny and Elkan 2001).\nOther types of outputs: Some examples are possibility theory (Dubois and Prade 2001), credal sets (Levi 1980), conformal predictions (Vovk, Gammerman, and Shafer 2005), multi-label (Alotaibi and Flach 2021).\n\n\n\n\n\n\nClassifier as a black box\n\n\n\n\n\n\n\n\nTraining vs Deployment",
    "crumbs": [
      "Introduction to Optimal Decision Making"
    ]
  },
  {
    "objectID": "cha_odm/odm.html#sec-cost",
    "href": "cha_odm/odm.html#sec-cost",
    "title": "Introduction to Optimal Decision Making",
    "section": "Cost-sensitive classification",
    "text": "Cost-sensitive classification\n\n\n\nCost-sensitive classification (Elkan 2001) provides a framework to make optimal decisions (with certain assumptions).\nWe require the true posterior probabilities of each outcome in order to make optimal decisions, but we can use estimates.\nAssumes the costs are not instance dependent (only depend on the predicted and true class).\nClass priors and costs can be changed during deployment (if known or estimated).\n\n\nCost matrices: Binary example\n\nThe following is a typical example of a cost matrix c for a binary problem.\n\n\n\n\nPredicted C_1\nPredicted C_2\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{1}\n\n\nTrue C_2\n\\color{DarkRed}{1}\n\\color{DarkGreen}{0}\n\n\n\nWe will refer to c_{i|j} the cost of predicting class C_i given that the true class is C_j.\n\n\nGiven the posterior probabilities P(C_j|\\mathbf{x}) where j \\in \\{1, K\\} and the cost matrix c we can calculate the expected cost of predicting class C_i\n\\begin{equation}\n  \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j}) = \\sum_{j = 1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\n\n\nFor example, lets assume that the posterior probability vector for a given instance is [0.4, 0.6], the expected costs will be\n\nPredicting Class 1 will have an expected cost of 0.4 \\times 0 +\n  0.6 \\times 1 = \\color{DarkRed}{0.6}\nPredicting Class 2 will have an expected cost of 0.4 \\times 1\n  + 0.6 \\times 0 = \\mathbf{\\color{DarkRed}{0.4}}.\n\n\n\nExpected costs figure\n\nWe can visualise the cost lines for each prediction with a line for each predicted class C_i and its missclassification costs and correct predictions (Drummond and Holte 2006). For example, the following cost matrix\n\n\n\n\nPredicted C_1\nPredicted C_2\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{1}\n\n\nTrue C_2\n\\color{DarkRed}{1}\n\\color{DarkGreen}{0}\n\n\n\n\n\nwill result in the following cost lines\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nC = [[0, 1], [1, 0]]\nthreshold = (C[0][1] - C[1][1])/(C[0][1] - C[1][1] + C[1][0] - C[0][0])\ncost_t = threshold*C[0][0] + (1-threshold)*C[0][1]\nplt.grid(True)\nplt.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\nplt.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\nplt.plot([threshold, 1], [cost_t, C[0][0]], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\nplt.plot([0, threshold], [C[1][1], cost_t], lw=5, color='tab:orange', label=\"Optimal $C_2$\")\nplt.xlabel('$P(C_1|x)$')\nplt.ylabel('Expected cost')\nplt.legend()\nplt.annotate(\"Optimal threshold = 0.5\", (0.5, 0.48), xytext=(0.4, 0.2),\n             arrowprops=dict(arrowstyle='-&gt;', facecolor='black'))\nplt.scatter(0.5, 0.5, s=100, facecolors='none', edgecolors='tab:red', zorder=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nwhere we have highlighted the minimum cost among the possible predictions. In this particular case the optimal prediction changes when the probability of the true class is higher or lower than 0.5, with the same expected cost for both classes at 0.5.\n\n\n\nCost Matrix “reasonableness” condition\n\nIn general, it is reasonable to expect cost matrices where:\n\n\n\n\nFor a given class j the correct prediction has always a lower cost than an incorrect prediction c_{j|j} &lt; c_{i|j} with i \\neq j.\nClass domination: One class does not consistently have lower costs than other classes c_{i|j} \\leq c_{k|j} for all j.\n\n\n\n\nWe will make these reasonable assumptions in this introductory module.\n\n\n\nClass Domination\n\nThe following is an example of class domination in which predicting class C_1 will always have a lower expected cost.\n\n\n\n\nPredicted C_1\nPredicted C_2\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{1}\n\n\nTrue C_2\n\\color{DarkRed}{0.4}\n\\color{DarkGreen}{0.5}\n\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nplt.grid(True)\nplt.plot([0, 1], [0.4, 0], '--', color='tab:blue', label=\"Predict $C_1$\")\nplt.plot([0, 1], [0.5, 1], '--', color='tab:orange', label=\"Predict $C_2$\")\nplt.plot([0, 1], [0.4, 0], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\nplt.xlabel('$P(C_1|x)$')\nplt.ylabel('Expected cost')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal threshold for the binary case\n\n\n\nIf we know the true posterior probabilities, the optimal decision is to choose the class that minimizes the expected cost which can be obtained by marginalising the predicted class over all possible true classes (O’Brien, Gupta, and Gray 2008).\n\\begin{equation}\n  \\hat{y}(\\mathbf{x}) = \\mathop{\\mathrm{arg\\,min}}_{i=\\{1, \\dots, K\\}} \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j})\n                   = \\mathop{\\mathrm{arg\\,min}}_{i=\\{1, \\dots, K\\}} \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\n\n\nIn the binary case we want to predict class C_1 if and only if predicting class C_1 has a lower expected cost than predicting class C_2\n\\begin{align}\n    \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{1|j}             &\\le \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{2|j} \\\\\n    P(C_1|\\mathbf{x}) c_{1|1} + P(C_2|\\mathbf{x}) c_{1|2} &\\le P(C_1|\\mathbf{x}) c_{2|1} + P(C_2|\\mathbf{x}) c_{2|2} \\\\\n\\end{align}\n\n\nwith the equality having the same expected cost independent on the predicted class.\n\\begin{equation}\np c_{1|1} + (1 - p) c_{1|2}                    =   p c_{2|1} + (1 - p) c_{2|2}\n\\end{equation}\nwhere p = P(C_1|\\mathbf{x}).\n\n\nIn the binary classification setting we can derive the optimal threshold t^* of selecting class one if p \\ge t^*.\n\\begin{align}\n    t^* c_{1|1} + (1 - t^*) c_{1|2}                 &=   t^* c_{2|1} + (1 - t^*) c_{2|2} \\\\\n    (1 - t^*) c_{1|2}  - (1 - t^*) c_{2|2}          &=   t^* c_{2|1} - t^* c_{1|1} \\\\\n    (1 - t^*) (c_{1|2}  - c_{2|2})                  &=   t^* (c_{2|1} - c_{1|1}) \\\\\n    (c_{1|2}  - c_{2|2}) -t^*(c_{1|2}  - c_{2|2})   &=   t^* (c_{2|1} - c_{1|1}) \\\\\n    (c_{1|2}  - c_{2|2})                            &=   t^* (c_{2|1} - c_{1|1}) + t^*(c_{1|2}  - c_{2|2}) \\\\\n    (c_{1|2}  - c_{2|2})                            &=   t^* (c_{2|1} - c_{1|1} + c_{1|2}  - c_{2|2}) \\\\\n    \\frac{c_{1|2}  - c_{2|2}}{c_{2|1} - c_{1|1} + c_{1|2}  - c_{2|2}} &=   t^*\n\\end{align}\n\nFor the previous cost matrix\n\n\n\n\nPredicted C_1\nPredicted C_2\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{1}\n\n\nTrue C_2\n\\color{DarkRed}{1}\n\\color{DarkGreen}{0}\n\n\n\nthe optimal threshold corresponds to\n\\begin{equation}\n  t^* = \\frac{{\\color{DarkRed}c_{1|2}} - {\\color{DarkGreen}c_{2|2}}}\n               {{\\color{DarkRed}c_{1|2}} - {\\color{DarkGreen}c_{2|2}} +\n                {\\color{DarkRed}c_{2|1}} - {\\color{DarkGreen}c_{1|1}}}\n  = \\frac{1 - 0}{1 + 1 - 0 - 0} = 0.5\n\\end{equation}\n\n\nDifferent costs binary example\nIn general, the correct predictions have a cost of 0. However, this may be different in certain scenarios. The following is an example of a cost matrix with different gains on the main diagonal and missclassification costs.\n\n\n\n\nPredicted C_1\nPredicted C_2\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{-5}\n\\color{DarkRed}{10}\n\n\nTrue C_2\n\\color{DarkRed}{1}\n\\color{DarkGreen}{-1}\n\n\n\nwhich would result in the following cost lines.\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nC = [[-5, 1],  # TP, FN\n     [10, -1]] # FP, TN\nthreshold = (C[0][1] - C[1][1])/(C[0][1] - C[1][1] + C[1][0] - C[0][0])\ncost_t = threshold*C[0][0] + (1-threshold)*C[0][1]\nplt.grid(True)\nplt.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\nplt.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\nplt.plot([threshold, 1], [cost_t, C[0][0]], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\nplt.plot([0, threshold], [C[1][1], cost_t], lw=5, color='tab:orange', label=\"Optimal $C_2$\")\nplt.xlabel('$P(C_1|x)$')\nplt.ylabel('Expected cost')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, for a posterior probability vector [0.4, 0.6] we would expect\n\nPredicting Class 1 will have an expected cost of -5 \\times 0.4 + 1 \\times\n0.6 = \\mathbf{\\color{DarkGreen}{-1.4}}\nPredicting Class 2 will have an expected cost of 10 \\times 0.4 - 1 \\times\n0.6 = \\color{DarkRed}{3.4}\n\n\n\nOther binary examples\nSee how the beginning and end of the cost lines change with the costs.\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nimport matplotlib.pyplot as plt\nfrom shiny import App, render, ui\nimport pandas as pd\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n    ui.input_slider(\"TP\", \"Cost True C1\",  value=-5, min=-10, max=0),\n    ui.input_slider(\"TN\", \"Cost True C2\",  value=-1, min=-10, max=0),\n    ui.input_slider(\"FN\", \"Cost False C2\", value=10, min=1,   max=10),\n    ui.input_slider(\"FP\", \"Cost False C1\", value=1,  min=1,   max=10),\n    ),\n    ui.panel_main(\n    ui.output_plot(\"plot\")\n    )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        TP = input.TP() # C_1|1\n        FN = input.FN() # C_1|2\n        FP = input.FP() # C_2|1\n        TN = input.TN() # C_2|2\n        fig = plt.figure()\n        ax = fig.add_subplot()\n        ax.grid(True)\n        ax.plot([0, 1], [FP, TP], '--', label=\"Predict $C_1$\")\n        ax.plot([0, 1], [TN, FN], '--', label=\"Predict $C_2$\")\n\n        threshold = (FP - TN)/(FP - TN + FN - TP)\n        cost_t = threshold*TP + (1-threshold)*FP\n        ax.plot([threshold, 1], [cost_t, TP], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\n        ax.plot([0, threshold], [TN, cost_t], lw=5, color='tab:orange', label=\"Optimal $C_2$\")\n\n        C = [[TP, FP], [FN, TN]]\n        bbox = dict(boxstyle=\"round\", fc=\"white\")\n        ax.annotate(r'$C_{2|2}$', (0, C[1][1]), xytext=(2, -1),\n                    textcoords='offset fontsize',\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|1}$', (1, C[0][0]), xytext=(2, 0),\n                    textcoords='offset fontsize',\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|2}$', (0, C[0][1]), xytext=(0, 2),\n                    textcoords='offset fontsize',\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{2|1}$', (1, C[1][0]), xytext=(2, 0),\n                    textcoords='offset fontsize',\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.annotate(f'$t*={threshold:0.2}$', (threshold, cost_t), \n                    xytext=(0, --3),\n                    textcoords='offset fontsize',\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.set_xlabel('$P(C_1|x)$')\n        ax.set_ylabel('Expected cost')\n        ax.legend()\n\n        return fig\n\napp = App(app_ui, server, debug=True)\n\n\n\nCost invariances\nThe optimal prediction does not change if the cost matrix is\n\nMultiplied by a positive constant value\nShifted by a constant value\n\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, render, ui\nimport pandas as pd\n\n\ndef fraction_to_float(fraction):\n    if '/' in fraction:\n       numerator, denominator = fraction.split('/') \n       result = float(numerator)/float(denominator)\n    else:\n       result = float(fraction)\n    return result\n\n# X|Y means predict X given that the true label is Y\n# Because the indices in a matrix are first row and then column we need to\n# invert the order of X and Y by transposing the matrix. Then [0,1] is predict 0\n# when the true label is 1.\n# TODO: check indices\nC_original = np.array([[-2,  3],     # 1|1, 2|1\n                       [13, -7]]).T  # 1|2, 2|2\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"S\", \"Shift constant S\", value=0,  min=-10,\n                            max=10),\n            ui.input_radio_buttons(\"M\", \"Multiplicative constant M\",\n                                   choices=['1/20', '1/10', '1/5', '1',\n                                            '5', '10', '20'],\n                                   selected = '1', inline=True, width='100%'),\n            ui.output_table('cost_matrix'),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"plot\")\n        )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        fig = plt.figure()\n        ax = fig.add_subplot()\n        ax.grid(True)\n\n        global C_original\n        C = C_original + input.S()\n        C = C*fraction_to_float(input.M())\n        \n        threshold = (C[0][1] - C[1][1])/(C[0][1] - C[1][1] + C[1][0] - C[0][0])\n        cost_t = threshold*C[0][0] + (1-threshold)*C[0][1]\n\n        ax.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\n        ax.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\n        ax.plot([threshold, 1], [cost_t, C[0][0]], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\n        ax.plot([0, threshold], [C[1][1], cost_t], lw=5, color='tab:orange', label=\"Optimal $C_2$\")\n\n        bbox = dict(boxstyle=\"round\", fc=\"white\")\n        ax.annotate(r'$C_{2|2}$', (0, C[1][1]), xytext=(-0.2, C[1][1]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|1}$', (1, C[0][0]), xytext=(1.1, C[0][0]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|2}$', (0, C[0][1]), xytext=(-0.2, C[0][1]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{2|1}$', (1, C[1][0]), xytext=(1.1, C[1][0]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.annotate(f'$t*={threshold:0.2}$', (threshold, cost_t), \n                    xytext=(threshold + 0.2, cost_t),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.set_xlabel('$P(C_1|x)$')\n        ax.set_ylabel('Expected cost')\n        ax.legend()\n\n        return fig\n    @output\n    @render.table(index=True)\n    def cost_matrix():\n        global C_original\n        C = C_original.T + input.S() # Need to transpose back to show print matrix\n        C = C*fraction_to_float(input.M())\n\n        return pd.DataFrame(C,\n                            index=['True C1', 'True C2'],\n                            columns=['Predicted C1', 'Predicted C2'])\n\napp = App(app_ui, server, debug=True)\n\n\nSimplification example\nBecause of these invariances, it is common in the binary case to modify the matrix c in such a way that the missclassification cost for one of the classes is 1 and a cost of 0 for its correct prediction. For example, if c_{1|2}^*=1 and c_{2|2}^*=0 we get\n\n\\begin{equation}\n  t^* = \\frac{{\\color{DarkRed}c_{1|2}} - {\\color{DarkGreen}c_{2|2}}}\n               {{\\color{DarkRed}c_{1|2}} - {\\color{DarkGreen}c_{2|2}} +\n                {\\color{DarkRed}c_{2|1}} - {\\color{DarkGreen}c_{1|1}}}\n  = \\frac{1}\n               {1 +\n                {\\color{DarkRed}c_{2|1}^*} - {\\color{DarkGreen}c_{1|1}^*}}\n\\end{equation}\n\n\nIn the previous example the original cost matrix c\n\\begin{equation}\nc = \\begin{bmatrix}-2 & 3 \\\\ 13 & -7\\end{bmatrix}^\\intercal\n\\end{equation}\n\n\nif shifted by +7 and scaled by 1/20 results in\n\\begin{equation}\nc' = \\begin{bmatrix} (-2 + 7)/20 & (3 + 7)/20 \\\\ (13 + 7)/20 & (-7 + 7)/20\\end{bmatrix}^\\intercal\n= \\begin{bmatrix}0.25 & 0.5 \\\\ 1 & 0\\end{bmatrix}^\\intercal\n\\end{equation}\nwith an optimal threshold\n\\begin{equation}\n  t^*\n  = \\frac{1} {1 + {\\color{DarkRed}c_{2|1}}' - {\\color{DarkGreen}c_{1|1}'}}\n  = \\frac{1} {1 + {\\color{DarkRed}0.5} - {\\color{DarkGreen}0.25}}\n  = 0.8\n\\end{equation}\n\n\n\n\nMulticlass setting\n\n\n\nThe binary cost matrix can be extended to multiclass by extending the rows with additional true classes and columns with predicted classes.\n\n\n\n\n\n\n\n\n\n\n\nPredicted C_1\nPredicted C_2\n\\cdots\nPredicted C_K\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{c_{1|1}}\n\\color{DarkRed}{c_{2|1}}\n\\cdots\n\\color{DarkRed}{c_{K|1}}\n\n\nTrue C_2\n\\color{DarkRed}{c_{1|2}}\n\\color{DarkGreen}{c_{2|2}}\n\\cdots\n\\color{DarkRed}{c_{2|2}}\n\n\n\\vdots\n\\vdots\n\\vdots\n\\ddots\n\\vdots\n\n\nTrue C_K\n\\color{DarkRed}{c_{1|K}}\n\\color{DarkRed}{c_{2|K}}\n\\cdots\n\\color{DarkGreen}{c_{K|K}}\n\n\n\n\n\nHowever, with more than 2 classes the threshold is not a single value but multiple decision boundaries in the probability simplex.\n\n\n\nTernary example {.smaller}\n\nIn order to exemplify the process of making an optimal decision in more with more than two classes we can look at the ternary case, which naturally extends to more classes. Given the following cost matrix\n\n\n\n\n\n\n\n\n\n\nPredicted C_1\nPredicted C_2\nPredicted C_3\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{-10}\n\\color{DarkRed}{20}\n\\color{DarkRed}{30}\n\n\nTrue C_2\n\\color{DarkRed}{40}\n\\color{DarkGreen}{-50}\n\\color{DarkRed}{60}\n\n\nTrue C_3\n\\color{DarkRed}{70}\n\\color{DarkRed}{80}\n\\color{DarkGreen}{-90}\n\n\n\n\n\nand a true posterior probability vector for all the classes [0.5, 0.1, 0.4], we can estimate the expected cost of making each class prediction\n\\begin{equation}\n  \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j}) = \\sum_{j = 1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\n\n\nwhich results in the following expected costs:\n\nPredicting Class 1 will have a cost of -10 \\times 0.5 + 40 \\times 0.1 +70 \\times 0.4 = \\color{DarkRed}{27}\nPredicting Class 2 will have a cost of 20 \\times 0.5 - 50 \\times 0.1 +80 \\times 0.4 = \\color{DarkRed}{37}\nPredicting Class 3 will have a cost of 30 \\times 0.5 + 60 \\times 0.1 -90 \\times 0.4 = \\mathbf{\\color{DarkGreen}{-15}}\n\n\n\n\nTernary expected cost isolines per decision\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom pycalib.visualisations.barycentric import draw_func_contours\n\nC = [[-10, 40, 70], [20, -50, 80], [30, 60, -90]]\n\ncmaps = ['Blues_r', 'Oranges_r', 'Greens_r']\nlabels = [f\"$P(C_{i+1}|x) = 1$\" for i in range(len(C))]\n\nfig = plt.figure(figsize=(10, 4))\nfor i in range(len(C)):\n    ax = fig.add_subplot(1, len(C), i+1)\n\n    def cost_func(prob):\n        return sum(prob*C[i])\n\n    ax.set_title(f\"Expected cost of predicting $C_{i+1}$\\n\")\n    draw_func_contours(cost_func, labels=labels, nlevels=10, subdiv=4,\n                       cmap=cmaps[i], fig=fig, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTernary hyperplanes optimal decision combined\n\n\n\nShow the code\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pycalib.visualisations.barycentric import draw_func_contours\n\nC = [[-10, 40, 70], [20, -50, 80], [30, 60, -90]]\n\ncmaps = ['Blues_r', 'Oranges_r', 'Greens_r']\nlabels = [f\"$P(C_{i+1}|x) = 1$\" for i in range(len(C))]\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot()\nfig.suptitle(f\"Expected cost optimal prediction\")\nfor i in range(len(C)):\n    def cost_func(prob):\n        expected_costs = np.inner(prob, C)\n        min_p_id = np.argmin(expected_costs)\n        if min_p_id == i:\n            return expected_costs[i]\n        return np.nan\n\n    draw_func_contours(cost_func, labels=labels, nlevels=10, subdiv=4,\n                       cmap=cmaps[i], cb_orientation='vertical', fig=fig, ax=ax)\n\n\nplt.show()\n\n\n/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/pycalib/visualisations/barycentric.py:140: UserWarning:\n\nThe following kwargs were not used by contour: 'cb_orientation'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption to abstain\nIt is possible to add the costs of abstaining on making a prediction by adding a column into the original cost matrix (Charoenphakdee et al. 2021). The following is an example which illustrates this in a binary classification problem.\n\n\n\n\n\n\n\n\n\n\nPredicted C_1\nPredicted C_2\nAbstain\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{10}\n\\color{DarkRed}{2}\n\n\nTrue C_2\n\\color{DarkRed}{9}\n\\color{DarkGreen}{-3}\n\\color{DarkRed}{2}\n\n\n\n\nPredicting Class 1 has an expected cost of 0 \\times 0.3 + 9 \\times 0.7 = \\color{DarkRed}{6.3}\nPredicting Class 2 has an expected cost of 10 \\times 0.3 - 3 \\times 0.7 = \\mathbf{\\color{DarkRed}{0.9}}\nAbstaining has an expected cost of 2 \\times 0.3 + 2 \\times 0.7 = \\color{DarkRed}{2}\n\n\n\nOption to abstain cost lines\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nC = [[0, 9], [10, -3], [2, 2]]\np = np.linspace(0, 1, 100)\np = np.vstack([1 - p, p]).T\nopt_cost = [min(np.inner(C, p[i])) for i in range(p.shape[0])]\nplt.plot(p[:,0], opt_cost, lw=5, label='Optimal')\n\nplt.grid(True)\nplt.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\nplt.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\nplt.plot([0, 1], [C[2][1], C[2][0]], '--', c='tab:red', label=\"Abstain\")\nplt.xlabel('$P(C_1|x)$')\nplt.ylabel('Expected cost')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nOption to abstain different costs\nThe following is another example in which abstaining from making a prediction if the true class was C_2 would incur into a gain.\n\n\n\n\n\n\n\n\n\n\nPredicted C_1\nPredicted C_2\nAbstain\n\n\n\n\nTrue C_1\n\\color{DarkGreen}{0}\n\\color{DarkRed}{10}\n\\color{DarkRed}{2}\n\n\nTrue C_2\n\\color{DarkRed}{9}\n\\color{DarkGreen}{-3}\n\\color{DarkGreen}{-1}\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nC = np.array([[0, 9], [10, -3], [2, -1]])\np = np.linspace(0, 1, 100)\np = np.vstack([1 - p, p]).T\nopt_cost = [min(np.inner(C, p[i])) for i in range(p.shape[0])]\nplt.plot(p[:,0], opt_cost, lw=5, label='Optimal')\n\nplt.grid(True)\nplt.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\nplt.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\nplt.plot([0, 1], [C[2][1], C[2][0]], '--', c='tab:red', label=\"Abstain\")\nplt.xlabel('$P(C_1|x)$')\nplt.ylabel('Expected cost')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Introduction to Optimal Decision Making"
    ]
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-decision-making.-why-and-how",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-decision-making.-why-and-how",
    "title": "Introduction to Optimal Decision Making",
    "section": "Optimal Decision Making. Why and how?",
    "text": "Optimal Decision Making. Why and how?\n\n\nThe objective is to classify a new instance into one of the possible classes in an optimal manner.\nThis may be important in critical applications: e.g. medical diagnosis (Begoli, Bhattacharya, and Kusnezov 2019; Yang, Steinfeld, and Zimmerman 2019), self-driving cars (Qayyum et al. 2020; Mullins et al. 2018), extreme weather prediction, finances (Nti, Adekoya, and Weyori 2020). \nIt is necessary to know what are the consequences of making each prediction (costs or gains).\nOne way to make optimal decisions is with cost-sensitive classification.\nCan we make optimal decisions with any type of classifier?"
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-decisions-with-different-types-of-model",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-decisions-with-different-types-of-model",
    "title": "Introduction to Optimal Decision Making",
    "section": "Optimal decisions with different types of model",
    "text": "Optimal decisions with different types of model\n\nClass estimation: Outputs a class prediction.\nClass estimation with option of abstaining: Outputs a class prediction or abstains (Coenen, Abdullah, and Guns 2020; Mozannar and Sontag 2020)\nRankings estimation: Outputs a ranked list of possible classes (Brinker and Hüllermeier 2020).\nScore surrogates: Outputs a continuous score which is commonly a surrogate for classification (e.g. Support Vector Machines).\nProbability estimation: Outputs class posterior probability estimates (e.g. Logistic Regression, naive Bayes, Artificial Neural Networks), or provides class counts which can be interpreted as proportions (e.g. decision trees, random forests, k-nearest neightbour) (Zadrozny and Elkan 2001).\nOther types of outputs: Some examples are possibility theory (Dubois and Prade 2001), credal sets (Levi 1980), conformal predictions (Vovk, Gammerman, and Shafer 2005), multi-label (Alotaibi and Flach 2021).\n\n\n\n\n\n\nClassifier as a black box\n\n\n\n\n\n\n\n\nTraining vs Deployment"
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-binary",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-binary",
    "title": "Introduction to Optimal Decision Making",
    "section": "Cost matrices: Binary example",
    "text": "Cost matrices: Binary example\n\nThe following is a typical example of a cost matrix \\(c\\) for a binary problem.\n\n\n\n\nPredicted \\(C_1\\)\nPredicted \\(C_2\\)\n\n\n\n\nTrue \\(C_1\\)\n\\(\\color{DarkGreen}{0}\\)\n\\(\\color{DarkRed}{1}\\)\n\n\nTrue \\(C_2\\)\n\\(\\color{DarkRed}{1}\\)\n\\(\\color{DarkGreen}{0}\\)\n\n\n\nWe will refer to \\(c_{i|j}\\) the cost of predicting class \\(C_i\\) given that the true class is \\(C_j\\).\n\n\nGiven the posterior probabilities \\(P(C_j|\\mathbf{x})\\) where \\(j \\in \\{1, K\\}\\) and the cost matrix \\(c\\) we can calculate the expected cost of predicting class \\(C_i\\)\n\\[\\begin{equation}\n  \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j}) = \\sum_{j = 1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\\]"
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#cost-matrix-reasonableness-condition",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#cost-matrix-reasonableness-condition",
    "title": "Introduction to Optimal Decision Making",
    "section": "Cost Matrix “reasonableness” condition",
    "text": "Cost Matrix “reasonableness” condition\n\nIn general, it is reasonable to expect cost matrices where:\n\n\n\n\nFor a given class \\(j\\) the correct prediction has always a lower cost than an incorrect prediction \\(c_{j|j} &lt; c_{i|j}\\) with \\(i \\neq j\\).\nClass domination: One class does not consistently have lower costs than other classes \\(c_{i|j} \\leq c_{k|j}\\) for all \\(j\\).\n\n\n\n\nWe will make these reasonable assumptions in this introductory module."
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-threshold-for-the-binary-case",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#optimal-threshold-for-the-binary-case",
    "title": "Introduction to Optimal Decision Making",
    "section": "Optimal threshold for the binary case",
    "text": "Optimal threshold for the binary case\n\nIf we know the true posterior probabilities, the optimal decision is to choose the class that minimizes the expected cost which can be obtained by marginalising the predicted class over all possible true classes (O’Brien, Gupta, and Gray 2008).\n\\[\\begin{equation}\n  \\hat{y}(\\mathbf{x}) = \\mathop{\\mathrm{arg\\,min}}_{i=\\{1, \\dots, K\\}} \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j})\n                   = \\mathop{\\mathrm{arg\\,min}}_{i=\\{1, \\dots, K\\}} \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\\]\n\n\nIn the binary case we want to predict class \\(C_1\\) if and only if predicting class \\(C_1\\) has a lower expected cost than predicting class \\(C_2\\)\n\\[\\begin{align}\n    \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{1|j}             &\\le \\sum_{j=1}^K P(C_j|\\mathbf{x}) c_{2|j} \\\\\n    P(C_1|\\mathbf{x}) c_{1|1} + P(C_2|\\mathbf{x}) c_{1|2} &\\le P(C_1|\\mathbf{x}) c_{2|1} + P(C_2|\\mathbf{x}) c_{2|2} \\\\\n\\end{align}\\]\n\n\nwith the equality having the same expected cost independent on the predicted class.\n\\[\\begin{equation}\np c_{1|1} + (1 - p) c_{1|2}                    =   p c_{2|1} + (1 - p) c_{2|2}\n\\end{equation}\\]\nwhere \\(p = P(C_1|\\mathbf{x})\\)."
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#different-costs-binary-example",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#different-costs-binary-example",
    "title": "Introduction to Optimal Decision Making",
    "section": "Different costs binary example",
    "text": "Different costs binary example\nIn general, the correct predictions have a cost of \\(0\\). However, this may be different in certain scenarios. The following is an example of a cost matrix with different gains on the main diagonal and missclassification costs.\n\n\n\n\nPredicted \\(C_1\\)\nPredicted \\(C_2\\)\n\n\n\n\nTrue \\(C_1\\)\n\\(\\color{DarkGreen}{-5}\\)\n\\(\\color{DarkRed}{10}\\)\n\n\nTrue \\(C_2\\)\n\\(\\color{DarkRed}{1}\\)\n\\(\\color{DarkGreen}{-1}\\)\n\n\n\nwhich would result in the following cost lines."
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#cost-invariances",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#cost-invariances",
    "title": "Introduction to Optimal Decision Making",
    "section": "Cost invariances",
    "text": "Cost invariances\nThe optimal prediction does not change if the cost matrix is\n\nMultiplied by a positive constant value\nShifted by a constant value\n\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom shiny import App, render, ui\nimport pandas as pd\n\n\ndef fraction_to_float(fraction):\n    if '/' in fraction:\n       numerator, denominator = fraction.split('/') \n       result = float(numerator)/float(denominator)\n    else:\n       result = float(fraction)\n    return result\n\n# X|Y means predict X given that the true label is Y\n# Because the indices in a matrix are first row and then column we need to\n# invert the order of X and Y by transposing the matrix. Then [0,1] is predict 0\n# when the true label is 1.\n# TODO: check indices\nC_original = np.array([[-2,  3],     # 1|1, 2|1\n                       [13, -7]]).T  # 1|2, 2|2\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"S\", \"Shift constant S\", value=0,  min=-10,\n                            max=10),\n            ui.input_radio_buttons(\"M\", \"Multiplicative constant M\",\n                                   choices=['1/20', '1/10', '1/5', '1',\n                                            '5', '10', '20'],\n                                   selected = '1', inline=True, width='100%'),\n            ui.output_table('cost_matrix'),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"plot\")\n        )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        fig = plt.figure()\n        ax = fig.add_subplot()\n        ax.grid(True)\n\n        global C_original\n        C = C_original + input.S()\n        C = C*fraction_to_float(input.M())\n        \n        threshold = (C[0][1] - C[1][1])/(C[0][1] - C[1][1] + C[1][0] - C[0][0])\n        cost_t = threshold*C[0][0] + (1-threshold)*C[0][1]\n\n        ax.plot([0, 1], [C[0][1], C[0][0]], '--', label=\"Predict $C_1$\")\n        ax.plot([0, 1], [C[1][1], C[1][0]], '--', label=\"Predict $C_2$\")\n        ax.plot([threshold, 1], [cost_t, C[0][0]], lw=5, color='tab:blue', label=\"Optimal $C_1$\")\n        ax.plot([0, threshold], [C[1][1], cost_t], lw=5, color='tab:orange', label=\"Optimal $C_2$\")\n\n        bbox = dict(boxstyle=\"round\", fc=\"white\")\n        ax.annotate(r'$C_{2|2}$', (0, C[1][1]), xytext=(-0.2, C[1][1]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|1}$', (1, C[0][0]), xytext=(1.1, C[0][0]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{1|2}$', (0, C[0][1]), xytext=(-0.2, C[0][1]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n        ax.annotate(r'$C_{2|1}$', (1, C[1][0]), xytext=(1.1, C[1][0]),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.annotate(f'$t*={threshold:0.2}$', (threshold, cost_t), \n                    xytext=(threshold + 0.2, cost_t),\n                    arrowprops=dict(arrowstyle='-&gt;', facecolor='black'),\n                    bbox=bbox)\n\n        ax.set_xlabel('$P(C_1|x)$')\n        ax.set_ylabel('Expected cost')\n        ax.legend()\n\n        return fig\n    @output\n    @render.table(index=True)\n    def cost_matrix():\n        global C_original\n        C = C_original.T + input.S() # Need to transpose back to show print matrix\n        C = C*fraction_to_float(input.M())\n\n        return pd.DataFrame(C,\n                            index=['True C1', 'True C2'],\n                            columns=['Predicted C1', 'Predicted C2'])\n\napp = App(app_ui, server, debug=True)"
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-multiclass",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-multiclass",
    "title": "Introduction to Optimal Decision Making",
    "section": "Multiclass setting",
    "text": "Multiclass setting\n\nThe binary cost matrix can be extended to multiclass by extending the rows with additional true classes and columns with predicted classes.\n\n\n\n\n\n\n\n\n\n\n\nPredicted \\(C_1\\)\nPredicted \\(C_2\\)\n\\(\\cdots\\)\nPredicted \\(C_K\\)\n\n\n\n\nTrue \\(C_1\\)\n\\(\\color{DarkGreen}{c_{1|1}}\\)\n\\(\\color{DarkRed}{c_{2|1}}\\)\n\\(\\cdots\\)\n\\(\\color{DarkRed}{c_{K|1}}\\)\n\n\nTrue \\(C_2\\)\n\\(\\color{DarkRed}{c_{1|2}}\\)\n\\(\\color{DarkGreen}{c_{2|2}}\\)\n\\(\\cdots\\)\n\\(\\color{DarkRed}{c_{2|2}}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\nTrue \\(C_K\\)\n\\(\\color{DarkRed}{c_{1|K}}\\)\n\\(\\color{DarkRed}{c_{2|K}}\\)\n\\(\\cdots\\)\n\\(\\color{DarkGreen}{c_{K|K}}\\)\n\n\n\n\n\nHowever, with more than 2 classes the threshold is not a single value but multiple decision boundaries in the probability simplex."
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-ternary",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-ternary",
    "title": "Introduction to Optimal Decision Making",
    "section": "Ternary example {.smaller}",
    "text": "Ternary example {.smaller}\n\nIn order to exemplify the process of making an optimal decision in more with more than two classes we can look at the ternary case, which naturally extends to more classes. Given the following cost matrix\n\n\n\n\n\n\n\n\n\n\nPredicted \\(C_1\\)\nPredicted \\(C_2\\)\nPredicted \\(C_3\\)\n\n\n\n\nTrue \\(C_1\\)\n\\(\\color{DarkGreen}{-10}\\)\n\\(\\color{DarkRed}{20}\\)\n\\(\\color{DarkRed}{30}\\)\n\n\nTrue \\(C_2\\)\n\\(\\color{DarkRed}{40}\\)\n\\(\\color{DarkGreen}{-50}\\)\n\\(\\color{DarkRed}{60}\\)\n\n\nTrue \\(C_3\\)\n\\(\\color{DarkRed}{70}\\)\n\\(\\color{DarkRed}{80}\\)\n\\(\\color{DarkGreen}{-90}\\)\n\n\n\n\n\nand a true posterior probability vector for all the classes \\([0.5, 0.1, 0.4]\\), we can estimate the expected cost of making each class prediction\n\\[\\begin{equation}\n  \\mathbb{E}_{j \\sim P(\\cdot|\\mathbf{x})} (c_{i|j}) = \\sum_{j = 1}^K P(C_j|\\mathbf{x}) c_{i|j}.\n\\end{equation}\\]\n\n\nwhich results in the following expected costs:\n\nPredicting Class 1 will have a cost of \\(-10 \\times 0.5 + 40 \\times 0.1 +70 \\times 0.4 = \\color{DarkRed}{27}\\)\nPredicting Class 2 will have a cost of \\(20 \\times 0.5 - 50 \\times 0.1 +80 \\times 0.4 = \\color{DarkRed}{37}\\)\nPredicting Class 3 will have a cost of \\(30 \\times 0.5 + 60 \\times 0.1 -90 \\times 0.4 = \\mathbf{\\color{DarkGreen}{-15}}\\)"
  },
  {
    "objectID": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-abstain",
    "href": "cha_odm/slides-intro-to-opt-dec-mak.html#sec-abstain",
    "title": "Introduction to Optimal Decision Making",
    "section": "Option to abstain",
    "text": "Option to abstain\nIt is possible to add the costs of abstaining on making a prediction by adding a column into the original cost matrix (Charoenphakdee et al. 2021). The following is an example which illustrates this in a binary classification problem.\n\n\n\n\n\n\n\n\n\n\nPredicted \\(C_1\\)\nPredicted \\(C_2\\)\nAbstain\n\n\n\n\nTrue \\(C_1\\)\n\\(\\color{DarkGreen}{0}\\)\n\\(\\color{DarkRed}{10}\\)\n\\(\\color{DarkRed}{2}\\)\n\n\nTrue \\(C_2\\)\n\\(\\color{DarkRed}{9}\\)\n\\(\\color{DarkGreen}{-3}\\)\n\\(\\color{DarkRed}{2}\\)\n\n\n\n\nPredicting Class 1 has an expected cost of \\(0 \\times 0.3 + 9 \\times 0.7 = \\color{DarkRed}{6.3}\\)\nPredicting Class 2 has an expected cost of \\(10 \\times 0.3 - 3 \\times 0.7 = \\mathbf{\\color{DarkRed}{0.9}}\\)\nAbstaining has an expected cost of \\(2 \\times 0.3 + 2 \\times 0.7 = \\color{DarkRed}{2}\\)"
  },
  {
    "objectID": "4_reasoning_learning_social_context.html",
    "href": "4_reasoning_learning_social_context.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the foundations, techniques, algorithms and tools for allowing autonomous AI agents to be social and act within societies. It will offer a breadth of understanding in technologies that allow building Social AI systems and a multidisciplinary take on this topic that will impact every aspect of our daily life in the future.\n\n\nStudents should be able to:\n\nComprehend that capturing the social aspects of human behaviour is essential in understanding how people think and how people react to each other, which is a fundamental step to developing reasoning algorithms that can operate effectively in social contexts.\nDemonstrate a good understanding of computational models of social reality. That is, how social contexts determine human behaviour through norms, practices, conventions, rituals and other rules of human social nature.\nUnderstand current methodologies to model social cognition, collaboration and teamwork.\nUnderstand /describe theoretical models for cooperation between agents. \nUnderstand the process of creating systems equipped with perception and social capabilities that allows them to adapt to different social contexts and learn from other agents in such environments.\nUnderstand how models of social reality generate emergent behaviour and the impact of such models in agent societies and social networks of multi-agent systems. \n\n\n\n\nStudents should be able to:\n\nCorrectly identify different ways to sense the environment and understand how to use off-the-shelf solutions and how to make sense of the captured data.\nExplore the creation of a simple Social AI System, using a perception technology whose data feeds into a reasoning mechanism that outputs social (and intelligent) acts in a context of choice.\nEvaluate social reasoning and learning algorithms in the form of simulations or with a human.\nAnalyse the solutions to a problem and critically think about the societal impact.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nDesign and manage individual projects.\nClearly and succinctly communicate their ideas to technical audiences."
  },
  {
    "objectID": "4_reasoning_learning_social_context.html#reasoning-and-learning-in-social-contexts",
    "href": "4_reasoning_learning_social_context.html#reasoning-and-learning-in-social-contexts",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the foundations, techniques, algorithms and tools for allowing autonomous AI agents to be social and act within societies. It will offer a breadth of understanding in technologies that allow building Social AI systems and a multidisciplinary take on this topic that will impact every aspect of our daily life in the future.\n\n\nStudents should be able to:\n\nComprehend that capturing the social aspects of human behaviour is essential in understanding how people think and how people react to each other, which is a fundamental step to developing reasoning algorithms that can operate effectively in social contexts.\nDemonstrate a good understanding of computational models of social reality. That is, how social contexts determine human behaviour through norms, practices, conventions, rituals and other rules of human social nature.\nUnderstand current methodologies to model social cognition, collaboration and teamwork.\nUnderstand /describe theoretical models for cooperation between agents. \nUnderstand the process of creating systems equipped with perception and social capabilities that allows them to adapt to different social contexts and learn from other agents in such environments.\nUnderstand how models of social reality generate emergent behaviour and the impact of such models in agent societies and social networks of multi-agent systems. \n\n\n\n\nStudents should be able to:\n\nCorrectly identify different ways to sense the environment and understand how to use off-the-shelf solutions and how to make sense of the captured data.\nExplore the creation of a simple Social AI System, using a perception technology whose data feeds into a reasoning mechanism that outputs social (and intelligent) acts in a context of choice.\nEvaluate social reasoning and learning algorithms in the form of simulations or with a human.\nAnalyse the solutions to a problem and critically think about the societal impact.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nDesign and manage individual projects.\nClearly and succinctly communicate their ideas to technical audiences."
  },
  {
    "objectID": "2_ai_paradigms_representations.html",
    "href": "2_ai_paradigms_representations.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the challenge of integrating different representations and paradigms for AI in order to enable both learning, reasoning and optimisation. The integrated representations are intended to engender trustworthiness.\n\n\nStudents should be able to:\n\nUnderstand the motivations for the need to integrate learning, reasoning and optimisation, and the role of prior knowledge and knowledge representation.\nUnderstand integrated representations for Trustworthy AI.\nUnderstand different paradigms that integrate different representations. In particular:\n\nStatistical relational AI: the integration of logic and probability/fuzziness for both reasoning and learning\nNeurosymbolic AI: integrating logic with neural networks to enable perception and reasoning\nKnowledge graphs, ontologies, graph neural networks and embeddings.\nConstraint satisfaction and optimisation techniques: integrating solvers and learners for better performance and for learning CSP models.\n\nApply the above methods in perception, spatial reasoning, natural language processing, vision, and other societal/industrial domains\n\n\n\n\nStudents should be able to:\n\nUse a wide variety of representations (graphical models, logic, neural networks, knowledge graphs) for both learning and reasoning\nDiscern the power and limitations of different types of representations.\nCombine different representations for a particular AI task.\nUse both knowledge and data for a particular AI problem.\nUnderstand and use the above-mentioned categories of techniques (StarAI, NeSy, CSP, Knowledge graphs, Ontologies (OWL/RDFS).\nUnderstand the limitations and challenges of the integrated representations and paradigms.\nUnderstand the trustworthiness of these techniques.\n\n\n\n\nStudents should be able to:\n\nWork effectively with experts in different learning, reasoning and optimisation paradigms.\nCollaborate with domain experts to identify suitable integrated learning, reasoning and optimisation techniques for Trustworthy AI."
  },
  {
    "objectID": "2_ai_paradigms_representations.html#ai-paradigms-and-representations",
    "href": "2_ai_paradigms_representations.html#ai-paradigms-and-representations",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the challenge of integrating different representations and paradigms for AI in order to enable both learning, reasoning and optimisation. The integrated representations are intended to engender trustworthiness.\n\n\nStudents should be able to:\n\nUnderstand the motivations for the need to integrate learning, reasoning and optimisation, and the role of prior knowledge and knowledge representation.\nUnderstand integrated representations for Trustworthy AI.\nUnderstand different paradigms that integrate different representations. In particular:\n\nStatistical relational AI: the integration of logic and probability/fuzziness for both reasoning and learning\nNeurosymbolic AI: integrating logic with neural networks to enable perception and reasoning\nKnowledge graphs, ontologies, graph neural networks and embeddings.\nConstraint satisfaction and optimisation techniques: integrating solvers and learners for better performance and for learning CSP models.\n\nApply the above methods in perception, spatial reasoning, natural language processing, vision, and other societal/industrial domains\n\n\n\n\nStudents should be able to:\n\nUse a wide variety of representations (graphical models, logic, neural networks, knowledge graphs) for both learning and reasoning\nDiscern the power and limitations of different types of representations.\nCombine different representations for a particular AI task.\nUse both knowledge and data for a particular AI problem.\nUnderstand and use the above-mentioned categories of techniques (StarAI, NeSy, CSP, Knowledge graphs, Ontologies (OWL/RDFS).\nUnderstand the limitations and challenges of the integrated representations and paradigms.\nUnderstand the trustworthiness of these techniques.\n\n\n\n\nStudents should be able to:\n\nWork effectively with experts in different learning, reasoning and optimisation paradigms.\nCollaborate with domain experts to identify suitable integrated learning, reasoning and optimisation techniques for Trustworthy AI."
  },
  {
    "objectID": "0_foundations_of_ai.html",
    "href": "0_foundations_of_ai.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic presents the foundations, scope, history and methodologies of AI.\n\n\nStudents should be able to:\n\nComprehend and compare the various definitions of AI.\nUnderstand/describe the history of AI and the eras into which it can be periodized.\nProperly position AI within computer science and analyse its links with other fields of science or philosophy (neuroscience, philosophy of mind, electrical/electronic engineering, mathematics, cognitive science).\nUnderstand and historically order the most important propositions in the philosophy of AI (e.g., Turing test, physical symbol system hypothesis, etc.).\nComprehend the specific relationship of AI with logic, applied maths, game theory and other areas of mathematics.\nCompare and discriminate between different AI methodological paradigms (symbolic, computational, etc.).\nUnderstand/describe the concept of the intelligent agent.\n\n\n\n\nStudents should be able to:\n\nApply their critical and analytical faculties, in order to argue about the comparative advantages/disadvantages of different methodological paradigms from the rich history of AI.\nClearly argue about similarities and differences between natural/human intelligence and artificial intelligence, given the current level of technological progress and potential near-future advances.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nClearly and succinctly communicate their ideas to technical and non-technical audiences."
  },
  {
    "objectID": "0_foundations_of_ai.html#foundations-of-artificial-intelligence",
    "href": "0_foundations_of_ai.html#foundations-of-artificial-intelligence",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic presents the foundations, scope, history and methodologies of AI.\n\n\nStudents should be able to:\n\nComprehend and compare the various definitions of AI.\nUnderstand/describe the history of AI and the eras into which it can be periodized.\nProperly position AI within computer science and analyse its links with other fields of science or philosophy (neuroscience, philosophy of mind, electrical/electronic engineering, mathematics, cognitive science).\nUnderstand and historically order the most important propositions in the philosophy of AI (e.g., Turing test, physical symbol system hypothesis, etc.).\nComprehend the specific relationship of AI with logic, applied maths, game theory and other areas of mathematics.\nCompare and discriminate between different AI methodological paradigms (symbolic, computational, etc.).\nUnderstand/describe the concept of the intelligent agent.\n\n\n\n\nStudents should be able to:\n\nApply their critical and analytical faculties, in order to argue about the comparative advantages/disadvantages of different methodological paradigms from the rich history of AI.\nClearly argue about similarities and differences between natural/human intelligence and artificial intelligence, given the current level of technological progress and potential near-future advances.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nClearly and succinctly communicate their ideas to technical and non-technical audiences."
  },
  {
    "objectID": "1_foundations_of_tai.html",
    "href": "1_foundations_of_tai.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the dimensions of Trustworthy AI: (i) Explainability, (ii) Safety, (iii) Fairness, (iv) Accountability and Reproducibility, (v) Privacy, and (vi) Sustainability.\n\n\nStudents should be able to understand/describe current discourse on the following questions:\n\nHow can we guarantee user trust in AI systems through explanation? How to formulate explanations as Machine-Human conversation depending on context and user expertise?\nHow to bridge the gap from safety engineering, formal methods, verification as well as validation to the way AI systems are built, used, and reinforced?\nHow can we build algorithms that respect fairness constraints by design through understanding causal influences among variables for dealing with bias-related issues?\nHow to uncover accountability gaps w.r.t. the attribution of AI-related harming of humans?\nCan we guarantee privacy while preserving the desired utility functions?\nIs there any chance to reduce energy consumption for a more sustainable AI and how can AI contribute to solving some of the big sustainability challenges that face humanity today (e.g. climate change)?\nHow to deal with properties and tradeoffs among multiple dimensions? For instance, accuracy vs. fairness, privacy vs. transparency, convenience vs. dignity, personalization vs. solidarity, efficiency vs. safety and sustainability.\n\n\n\n\nStudents should be able to:\n\napply their critical and analytical faculties on specific case studies, in order to argue about the need and content of AI trustworthiness issues.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nClearly and succinctly communicate their ideas to technical and non-technical audiences."
  },
  {
    "objectID": "1_foundations_of_tai.html#foundations-of-trustworthy-ai",
    "href": "1_foundations_of_tai.html#foundations-of-trustworthy-ai",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers the dimensions of Trustworthy AI: (i) Explainability, (ii) Safety, (iii) Fairness, (iv) Accountability and Reproducibility, (v) Privacy, and (vi) Sustainability.\n\n\nStudents should be able to understand/describe current discourse on the following questions:\n\nHow can we guarantee user trust in AI systems through explanation? How to formulate explanations as Machine-Human conversation depending on context and user expertise?\nHow to bridge the gap from safety engineering, formal methods, verification as well as validation to the way AI systems are built, used, and reinforced?\nHow can we build algorithms that respect fairness constraints by design through understanding causal influences among variables for dealing with bias-related issues?\nHow to uncover accountability gaps w.r.t. the attribution of AI-related harming of humans?\nCan we guarantee privacy while preserving the desired utility functions?\nIs there any chance to reduce energy consumption for a more sustainable AI and how can AI contribute to solving some of the big sustainability challenges that face humanity today (e.g. climate change)?\nHow to deal with properties and tradeoffs among multiple dimensions? For instance, accuracy vs. fairness, privacy vs. transparency, convenience vs. dignity, personalization vs. solidarity, efficiency vs. safety and sustainability.\n\n\n\n\nStudents should be able to:\n\napply their critical and analytical faculties on specific case studies, in order to argue about the need and content of AI trustworthiness issues.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nClearly and succinctly communicate their ideas to technical and non-technical audiences."
  },
  {
    "objectID": "3_deciding_learning_how_act.html",
    "href": "3_deciding_learning_how_act.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers ways in which AI agents can be empowered with the ability to deliberate autonomously how to act in the world.\n\n\nStudents should be able to:\n\nUnderstand the different approaches in the fields of Artificial Intelligence and Formal Methods that can be applied in synergy to develop autonomous agents.\nRecognize the mathematical and algorithmic techniques as well as the key challenges to solving sequential decision-making problems.\nIntegrate data-driven learning methods with model-based reasoning methods for deciding and learning how to act.\nIdentify the limitations of current machine learning and reasoning methods to act in the real world.\n\n\n\n\nStudents should be able to:\n\nProgram advanced agents using learning and planning techniques for solving sequential decision-making tasks that involve other agents.\nAnalyse autonomy in dynamic, partially observable settings involving a single agent or multiple agents.\nDevelop methods for optimising control policies in complex sequential decision-making problems.\nImplement techniques to balance exploration and exploitation in decision-making tasks that require learning from the environment while acting on it.\nUse linear time logic as a specification language for formulating complex tasks as well as environment properties.\nApply synthesis from formal specifications to solve planning problems in nondeterministic environments.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team to reach a collective objective by sharing knowledge, learning and building consensus.\nPresent materials coherently and concisely in written or oral form, with clear use of language to a technical audience."
  },
  {
    "objectID": "3_deciding_learning_how_act.html#deciding-and-learning-how-to-act",
    "href": "3_deciding_learning_how_act.html#deciding-and-learning-how-to-act",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers ways in which AI agents can be empowered with the ability to deliberate autonomously how to act in the world.\n\n\nStudents should be able to:\n\nUnderstand the different approaches in the fields of Artificial Intelligence and Formal Methods that can be applied in synergy to develop autonomous agents.\nRecognize the mathematical and algorithmic techniques as well as the key challenges to solving sequential decision-making problems.\nIntegrate data-driven learning methods with model-based reasoning methods for deciding and learning how to act.\nIdentify the limitations of current machine learning and reasoning methods to act in the real world.\n\n\n\n\nStudents should be able to:\n\nProgram advanced agents using learning and planning techniques for solving sequential decision-making tasks that involve other agents.\nAnalyse autonomy in dynamic, partially observable settings involving a single agent or multiple agents.\nDevelop methods for optimising control policies in complex sequential decision-making problems.\nImplement techniques to balance exploration and exploitation in decision-making tasks that require learning from the environment while acting on it.\nUse linear time logic as a specification language for formulating complex tasks as well as environment properties.\nApply synthesis from formal specifications to solve planning problems in nondeterministic environments.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team to reach a collective objective by sharing knowledge, learning and building consensus.\nPresent materials coherently and concisely in written or oral form, with clear use of language to a technical audience."
  },
  {
    "objectID": "5_automated_ai.html",
    "href": "5_automated_ai.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers meta-level methods to ensure that AI tools and systems are performant, robust and trustworthy.\n\n\nStudents should be able to:\n\nExplain the basic problems solved by AutoAI methods, including (but not limited to) automated algorithm configuration, automated algorithm selection, automated performance prediction, model selection, hyperparameter optimisation and neural architecture search.\nExplain, in general and using specific examples, the significance of AutoAI problems and methods for the broader field of AI, including (but not limited to) the areas of machine learning, automated reasoning and optimisation.\n\nIn addition, students should be able to achieve a selection of the following, more specific learning outcomes:\n\nDemonstrate a working knowledge of Neural Architecture Search, notably how to define search spaces and optimise over these spaces, with both differential and black box methods.\nIdentify and define a Hyperparameter Optimization Problem (HPO), specifically in the domain of Algorithm Configuration and Neural Architecture Search. They should also be familiar with hyperparameter importance techniques to interpret different solutions to these problems.\nBe familiar with Gaussian Processes and their modelling capabilities, specifically in the domain of algorithm configuration.\nAssess the strengths and weaknesses of various HPO methods, notably bayesian and evolutionary strategies for doing such.\nDemonstrate knowledge on various speedup techniques to HPO, including leveraging previous information through meta-learning, learning curve prediction and bandit based scheduling techniques.\nDefine multiple objectives for an optimization problem and various evolutionary and bayesian techniques for solving such problems.\nExplain Dynamic Algorithm Configuration (DAC) and its difference to Static Algorithm Configuration. They should also be able to demonstrate how to use Reinforcement Learning to solve such optimization problems in DAC.\nDemonstrate knowledge of AutoAI methods for tasks that go beyond supervised learning. This includes knowledge of the underlying theoretical principles and algorithms as well as knowledge of specific tools and systems, including their correct and effective use, strengths and limitations.\nDemonstrate knowledge of AutoAI methods, tools and systems for problems in areas outside of machine learning (i.e., knowledge beyond automated machine learning).\nUnderstand the way how AI systems interact with their environment, and what are possible pitfalls of that (e.g., badly calibrated confidence statements, adversarial examples, (un)explainable decisions)\nExplain the importance for AI tools and systems to be able to detect situations in which their use becomes problematic (e.g., ineffective or unsafe).\nDemonstrate knowledge of techniques and approaches for achieving self-monitoring in at least one major area of AI.\nEvaluate AI systems for safety problems in interacting with their environments\nDemonstrate awareness of meta-learning, transfer learning, and continual learning techniques that can be leveraged to transfer information from earlier tasks.\nExplain how this transfer of knowledge can be used to make AutoML techniques and systems more efficient.\n\n\n\n\nStudents should be able to:\n\nCorrectly use a range of AutoAI techniques in at least one major area of AI.\nCritically assess (in technical and general ways) and explain the limitations of AutoAI methods.\nRecognise and explain technical problems that may arise in the use of AutoAI methods.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nDesign and manage individual projects.\nClearly and succinctly communicate their ideas to technical audiences."
  },
  {
    "objectID": "5_automated_ai.html#automated-ai",
    "href": "5_automated_ai.html#automated-ai",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "Content from TAILOR deliverable report D9.6\n\n\n\n\n\nThe content of this page is currently a re-formatted copy from the Deliverable 9.6 PhD Curriculum Report.\n\n\n\nThis topic covers meta-level methods to ensure that AI tools and systems are performant, robust and trustworthy.\n\n\nStudents should be able to:\n\nExplain the basic problems solved by AutoAI methods, including (but not limited to) automated algorithm configuration, automated algorithm selection, automated performance prediction, model selection, hyperparameter optimisation and neural architecture search.\nExplain, in general and using specific examples, the significance of AutoAI problems and methods for the broader field of AI, including (but not limited to) the areas of machine learning, automated reasoning and optimisation.\n\nIn addition, students should be able to achieve a selection of the following, more specific learning outcomes:\n\nDemonstrate a working knowledge of Neural Architecture Search, notably how to define search spaces and optimise over these spaces, with both differential and black box methods.\nIdentify and define a Hyperparameter Optimization Problem (HPO), specifically in the domain of Algorithm Configuration and Neural Architecture Search. They should also be familiar with hyperparameter importance techniques to interpret different solutions to these problems.\nBe familiar with Gaussian Processes and their modelling capabilities, specifically in the domain of algorithm configuration.\nAssess the strengths and weaknesses of various HPO methods, notably bayesian and evolutionary strategies for doing such.\nDemonstrate knowledge on various speedup techniques to HPO, including leveraging previous information through meta-learning, learning curve prediction and bandit based scheduling techniques.\nDefine multiple objectives for an optimization problem and various evolutionary and bayesian techniques for solving such problems.\nExplain Dynamic Algorithm Configuration (DAC) and its difference to Static Algorithm Configuration. They should also be able to demonstrate how to use Reinforcement Learning to solve such optimization problems in DAC.\nDemonstrate knowledge of AutoAI methods for tasks that go beyond supervised learning. This includes knowledge of the underlying theoretical principles and algorithms as well as knowledge of specific tools and systems, including their correct and effective use, strengths and limitations.\nDemonstrate knowledge of AutoAI methods, tools and systems for problems in areas outside of machine learning (i.e., knowledge beyond automated machine learning).\nUnderstand the way how AI systems interact with their environment, and what are possible pitfalls of that (e.g., badly calibrated confidence statements, adversarial examples, (un)explainable decisions)\nExplain the importance for AI tools and systems to be able to detect situations in which their use becomes problematic (e.g., ineffective or unsafe).\nDemonstrate knowledge of techniques and approaches for achieving self-monitoring in at least one major area of AI.\nEvaluate AI systems for safety problems in interacting with their environments\nDemonstrate awareness of meta-learning, transfer learning, and continual learning techniques that can be leveraged to transfer information from earlier tasks.\nExplain how this transfer of knowledge can be used to make AutoML techniques and systems more efficient.\n\n\n\n\nStudents should be able to:\n\nCorrectly use a range of AutoAI techniques in at least one major area of AI.\nCritically assess (in technical and general ways) and explain the limitations of AutoAI methods.\nRecognise and explain technical problems that may arise in the use of AutoAI methods.\n\n\n\n\nStudents should be able to:\n\nWork effectively with others in an interdisciplinary and/or international team.\nDesign and manage individual projects.\nClearly and succinctly communicate their ideas to technical audiences."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#weather-forecasters",
    "href": "cha_wahcc/slides-cla-cal.html#weather-forecasters",
    "title": "Classifier Calibration",
    "section": "Weather forecasters",
    "text": "Weather forecasters\n\nWeather forecasters started thinking about calibration a long time ago (Brier 1950).\n\nA forecast 70% chance of rain should be followed by rain 70% of the time.\n\nThis is immediately applicable to binary classification:\n\nA prediction 70% chance of spam should be spam 70% of the time.\n\nand to multi-class classification:\n\nA prediction 70% chance of setosa, 10% chance of versicolor and 20% chance of virginica should be setosa/versicolor/virginica 70/10/20% of the time.\n\nIn general:\n\nA predicted probability (vector) should match empirical (observed) probabilities.\n\n\n\n\n\n\n\n\nQ: What does X% of the time mean?\n\n\nIt means that we expect the occurrence of an event to happen “X%” of the time."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#forecasting-example",
    "href": "cha_wahcc/slides-cla-cal.html#forecasting-example",
    "title": "Classifier Calibration",
    "section": "Forecasting example",
    "text": "Forecasting example\nLet’s consider a small toy example:\n\nTwo predictions of 10% chance of rain were both followed by no rain.\nTwo predictions of 40% chance of rain were once followed by no rain, and once by rain.\nThree predictions of 70% chance of rain were once followed by no rain, and twice by rain.\nOne prediction of 90% chance of rain was followed by rain.\n\n\n\n\n\n\n\nQ: Is this forecaster well-calibrated?\n\n\nThe evaluation of calibration requires a large number of samples to make a statement. However, in this toy example we can assume that a \\(10\\%\\) discrepancy is acceptable, and that the number of samples is sufficient."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#over--and-under-estimates",
    "href": "cha_wahcc/slides-cla-cal.html#over--and-under-estimates",
    "title": "Classifier Calibration",
    "section": "Over- and under-estimates",
    "text": "Over- and under-estimates\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\nThis forecaster is doing a pretty decent job:\n\n10% chance of rain was a slight over-estimate\n(\\(\\bar{y} = 0/2 =   0\\%\\));\n40% chance of rain was a slight under-estimate\n(\\(\\bar{y} = 1/2 =  50\\%\\));\n70% chance of rain was a slight over-estimate\n(\\(\\bar{y} = 2/3 =  67\\%\\));\n90% chance of rain was a slight under-estimate\n(\\(\\bar{y} = 1/1 = 100\\%\\))."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#visualising-forecasts-the-reliability-diagram",
    "href": "cha_wahcc/slides-cla-cal.html#visualising-forecasts-the-reliability-diagram",
    "title": "Classifier Calibration",
    "section": "Visualising forecasts: the reliability diagram",
    "text": "Visualising forecasts: the reliability diagram\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#changing-the-numbers-slightly",
    "href": "cha_wahcc/slides-cla-cal.html#changing-the-numbers-slightly",
    "title": "Classifier Calibration",
    "section": "Changing the numbers slightly",
    "text": "Changing the numbers slightly\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n0.1\n0.2\n0\n0\n\n\n2\n3\n0.3\n0.4\n0\n1\n\n\n4\n5\n6\n0.6\n0.7\n0.8\n0\n1\n1\n\n\n7\n0.9\n1"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#or-should-we-group-the-forecasts-differently",
    "href": "cha_wahcc/slides-cla-cal.html#or-should-we-group-the-forecasts-differently",
    "title": "Classifier Calibration",
    "section": "Or should we group the forecasts differently?",
    "text": "Or should we group the forecasts differently?\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n2\n3\n0.1\n0.2\n0.3\n0.4\n0\n0\n0\n1\n\n\n4\n5\n6\n7\n0.6\n0.7\n0.8\n0.9\n0\n1\n1\n1"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#or-not-at-all",
    "href": "cha_wahcc/slides-cla-cal.html#or-not-at-all",
    "title": "Classifier Calibration",
    "section": "Or not at all?",
    "text": "Or not at all?\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n0.1\n0\n\n\n1\n0.2\n0\n\n\n2\n0.3\n0\n\n\n3\n0.4\n1\n\n\n4\n0.6\n0\n\n\n5\n0.7\n1\n\n\n6\n0.8\n1\n\n\n7\n0.9\n1"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#binning-or-pooling-predictions-is-a-fundamental-notion",
    "href": "cha_wahcc/slides-cla-cal.html#binning-or-pooling-predictions-is-a-fundamental-notion",
    "title": "Classifier Calibration",
    "section": "Binning or pooling predictions is a fundamental notion",
    "text": "Binning or pooling predictions is a fundamental notion\nWe need bins to evaluate the degree of calibration:\n\nIn order to decide whether a weather forecaster is well-calibrated, we need to look at a good number of forecasts, say over one year.\nWe also need to make sure that there are a reasonable number of forecasts for separate probability values, so we can obtain reliable empirical estimates.\n\nTrade-off: large bins give better empirical estimates, small bins allows a more fine-grained assessment of calibration.}\n\n\nBut adjusting forecasts in groups also gives rise to practical calibration methods:\n\nempirical binning\nisotonic regression (aka ROC convex hull)"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#expected-calibration-error-affected-by-the-number-of-bins",
    "href": "cha_wahcc/slides-cla-cal.html#expected-calibration-error-affected-by-the-number-of-bins",
    "title": "Classifier Calibration",
    "section": "Expected calibration error affected by the number of bins",
    "text": "Expected calibration error affected by the number of bins\nIn the following example you can change the number of bins of a fixed set of samples and see how the Expected Calibration Error changes accordingly.\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nfrom utils import plot_reliability_diagram\nimport numpy as np\n\nnp.random.seed(42)\nscores = np.random.rand(100)\nlabels = np.random.binomial(1, scores)\n\nimport numpy as np\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n    ui.input_slider(\"n\", \"Number of bins\", \n                    min=1, max=100, value=10),\n    ),\n    ui.panel_main(\n    ui.output_plot(\"plot\")\n    )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        fig = plot_reliability_diagram(labels, scores,\n                                       n_bins=input.n())\n        return fig\n\napp = App(app_ui, server, debug=True)\n\n\n## file: utils.py\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\nimport matplotlib.ticker as mticker\n\ndef get_binned_scores(labels, scores, bins=10):\n    '''\n    Parameters\n    ==========\n    labels : array (n_samples, )\n        Labels indicating the true class.\n    scores : matrix (n_samples, )\n        Output probability scores for one or several methods.\n    bins : int or list of floats\n        Number of bins to create in the scores' space, or list of bin\n        boundaries.\n    '''\n    n_bins = 1\n    if isinstance(bins, int):\n        n_bins = bins\n        bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n    elif isinstance(bins, list) or isinstance(bins, np.ndarray):\n        n_bins = len(bins) - 1\n        bins = np.array(bins)\n        if bins[0] == 0.0:\n            bins[0] = 0 - 1e-8\n        if bins[-1] == 1.0:\n            bins[-1] = 1 + 1e-8\n\n    scores = np.clip(scores, a_min=0, a_max=1)\n\n    bin_idx = np.digitize(scores, bins) - 1\n\n    bin_true = np.bincount(bin_idx, weights=labels,\n                           minlength=n_bins)\n    bin_pred = np.bincount(bin_idx, weights=scores,\n                           minlength=n_bins)\n    bin_total = np.bincount(bin_idx, minlength=n_bins)\n\n    zero_idx = bin_total == 0\n    avg_true = np.empty(bin_total.shape[0])\n    avg_true.fill(np.nan)\n    avg_true[~zero_idx] = np.divide(bin_true[~zero_idx],\n                                    bin_total[~zero_idx])\n    avg_pred = np.empty(bin_total.shape[0])\n    avg_pred.fill(np.nan)\n    avg_pred[~zero_idx] = np.divide(bin_pred[~zero_idx],\n                                    bin_total[~zero_idx])\n    return avg_true, avg_pred, bin_true, bin_total\n    \ndef plot_reliability_diagram(labels, scores, n_bins=10):\n    bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n\n    avg_true, avg_pred, bin_true, bin_total = get_binned_scores(\n            labels, scores, bins=bins)\n\n    zero_idx = bin_total == 0\n    fig = plt.figure()\n    ax1 = fig.add_subplot()\n    fig, axs = plt.subplots(2, 1,\n                            gridspec_kw={'height_ratios': [4, 1]})\n    axs[0].bar(x=bins[:-1][~zero_idx], height=avg_true[~zero_idx],\n                        align='edge', width=(bins[1:] - bins[:-1])[~zero_idx],\n                        edgecolor='black')\n    axs[0].scatter(avg_pred, avg_true)\n    axs[0].plot([0, 1], [0, 1], '--', color='red')\n\n    axs[0].set_xlim([0, 1])\n    axs[0].set_ylim([0, 1])\n    axs[0].set_ylabel('Fraction of positives')\n    axs[0].grid(True)\n    axs[0].set_axisbelow(True)\n\n    axs[1].hist(scores, range=(0, 1),\n             bins=bins,\n             histtype=\"bar\",\n             lw=1,\n             edgecolor='black')\n\n    axs[1].set_xlim([0, 1])\n    axs[1].set_ylabel('Count')\n    axs[1].set_xlabel('Scores')\n\n\n    weights = bin_total[~zero_idx]/sum(bin_total)\n    ece = sum(np.abs(avg_pred - avg_true)[~zero_idx]*weights)\n    fig.suptitle(f'Expected Calibration Error = {ece:0.3f}')\n    return fig"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#questions-and-answers",
    "href": "cha_wahcc/slides-cla-cal.html#questions-and-answers",
    "title": "Classifier Calibration",
    "section": "Questions and answers",
    "text": "Questions and answers\nQ&A 1\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times but it does not rain, two times 0.4 and it rains only once, five times 0.6 and it rains 80% of the times and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\nAnswer: Yes\n\n\nCorrect. You can see that there is one bin per predicted score \\(0.1, 0.4, 0.6\\) and \\(0.9\\). Each bin contains the number of scores indicated in the smaller histogram below with 2, 2, 5 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 0%, 50%, 80% and 100%.\n\n\n\n\n\n\nAnswer: No\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 2\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times and it rains once, three times 0.4 and it rains two times, four times 0.6 and it rains once and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\nAnswer: Yes\n\n\nCorrect. You can see that there is one bin per predicted score \\(0.1, 0.4, 0.6\\) and 0.9. Each bin contains the number of scores indicated in the smaller histogram below with 2, 3, 4 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 50%, 66.6%, 25% and 100%.\n\n\n\n\n\n\nAnswer: No\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 3\n\n\n\n\n\n\nQuestion 3\n\n\nDo we need multiple instances in each bin in order to visualise a reliability diagram?\n\n\n\n\n\n\nAnswer: Yes\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No\n\n\nCorrect. It is not necessary to have multiple instances in each bin for visualisation purposes. However, the lack of information does not allow us to know if the model is calibrated for those scores.\n\n\n\nQ&A 4\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\nCorrect. For each predicted score the actual fraction of positives is higher.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 5\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\nCorrect. For each predicted score the actual fraction of positives is lower.\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\nIncorrect. Try another answer."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#why-are-we-interested-in-calibration-1",
    "href": "cha_wahcc/slides-cla-cal.html#why-are-we-interested-in-calibration-1",
    "title": "Classifier Calibration",
    "section": "Why are we interested in calibration?",
    "text": "Why are we interested in calibration?\nTo calibrate means to employ a known scale with known properties.\n\nE.g., additive scale with a well-defined zero, so that ratios are meaningful.\n\nFor classifiers we want to use the probability scale, so that we can\n\njustifiably use default decision rules (e.g., maximum posterior probability);\nadjust these decision rules in a straightforward way to account for different class priors or misclassification costs;\ncombine probability estimates in a well-founded way.\n\n\n\n\n\n\n\nQ: Is the probability scale additive?\n\n\nIn some situations we may want to sum probabilities, for example if we have a set of mutually exclusive events, the probability of at least one of them happening can be computed by their sum. In other situations the product of probabilities is used, e.g. the probability of two independent events happening at the same time.\n\n\n\n\n\n\n\n\n\nQ: How would you combine probability estimates from several well-calibrated models?\n\n\nCheck some online information e.g. When pooling forecasts, use the geometric mean of odds\nAnd the following code shows some examples.\n\n\n\n\n\n\n\nTable 1: Example of probability means\n\n\n\n\n\n\nProbabilities\nArithmetic mean\nGeometric mean\nHarmonic mean\n\n\n\n\n[0.1 0.1]\n0.1\n0.1\n0.1\n\n\n[0.5 0.5]\n0.5\n0.5\n0.5\n\n\n[0.1 0.9]\n0.5\n0.5\n0.18\n\n\n[0.1 0.1 0.9]\n0.37\n0.32\n0.14\n\n\n[0.1 0.1 0.99]\n0.4\n0.52\n0.14\n\n\n[0.1 0.1 0.999]\n0.4\n0.7\n0.14"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#optimal-decisions-i",
    "href": "cha_wahcc/slides-cla-cal.html#optimal-decisions-i",
    "title": "Classifier Calibration",
    "section": "Optimal decisions I",
    "text": "Optimal decisions I\nDenote the cost of predicting class \\(j\\) for an instance of true class \\(i\\) as \\(C(\\hat{Y}=j|Y=i)\\). The expected cost of predicting class \\(j\\) for instance \\(x\\) is\n\\[\nC(\\hat{Y}=j|X=x) = \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\\]\nwhere \\(P(Y=i|X=x)\\) is the probability of instance \\(x\\) having true class \\(i\\) (as would be given by the Bayes-optimal classifier).\nThe optimal decision is then to predict the class with lowest expected cost:\n\\[\n\\hat{Y}^* = \\mathop{\\mathrm{arg\\,min}}_j C(\\hat{Y}=j|X=x) = \\mathop{\\mathrm{arg\\,min}}_j \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#optimal-decisions-ii",
    "href": "cha_wahcc/slides-cla-cal.html#optimal-decisions-ii",
    "title": "Classifier Calibration",
    "section": "Optimal decisions II",
    "text": "Optimal decisions II\nIn binary classification we have:\n\\[\\begin{align*}\nC(\\hat{Y}=+|X=x) &= P(+|x)C(+|+) + \\big(1-P(+|x)\\big)C(+|-) \\\\\nC(\\hat{Y}=-|X=x) &= P(+|x)C(-|+) + \\big(1-P(+|x)\\big)C(-|-)\n\\end{align*}\\]\nOn the optimal decision boundary these two expected costs are equal, which gives\n\\[\\begin{align*}%\\label{eq::cost-threshold}\nP(+|x)  = \\frac{\\color{blue}{C(+|-)-C(-|-)}}{\\color{blue}{C(+|-)-C(-|-)}+\\color{red}{C(-|+)-C(+|+)}} \\triangleq c\n\\end{align*}\\]\nThis gives the optimal threshold on the hypothetical Bayes-optimal probabilities.\nIt is also the best thing to do in practice – as long as the probabilities are well-calibrated!"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#optimal-decisions-iii",
    "href": "cha_wahcc/slides-cla-cal.html#optimal-decisions-iii",
    "title": "Classifier Calibration",
    "section": "Optimal decisions III",
    "text": "Optimal decisions III\nWithout loss of generality we can set the cost of true positives and true negatives to zero; \\(c = \\frac{c_{\\text{FP}}}{c_{\\text{FP}} + c_{\\text{FN}}}\\) is then the cost of a false positive in proportion to the combined cost of one false positive and one false negative.\n\nE.g., if false positives are 4 times as costly as false negatives then we set the decision threshold to \\(4/(4+1)=0.8\\) in order to only make positive predictions if we’re pretty certain.\n\nSimilar reasoning applies to changes in class priors:\n\nif we trained on balanced classes but want to deploy with 4 times as many positives compared to negatives, we lower the decision threshold to \\(0.2\\);\nmore generally, if we trained for class ratio \\(r\\) and deploy for class ratio \\(r'\\) we set the decision threshold to \\(r/(r+r')\\).\n\nCost and class prior changes can be combined in the obvious way."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#questions-and-answers-1",
    "href": "cha_wahcc/slides-cla-cal.html#questions-and-answers-1",
    "title": "Classifier Calibration",
    "section": "Questions and answers",
    "text": "Questions and answers\nQ&A 1\n\n\n\n\n\n\nQuestion 1\n\n\nIs it possible to compute optimal risks given a cost matrix and a probabilistic classifier that is not calibrated?\n\n\n\n\n\n\nAnswer: Yes\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No\n\n\nCorrect.\n\n\n\nQ&A 2\n\n\n\n\n\n\nQuestion\n\n\nGiven a calibrated probabilistic classifier, is it optimal to select the class with the highest predicted probability?\n\n\n\n\n\n\nAnswer: Yes\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nIf we have the following cost matrix, and a model outputs 0.6 probability for class 1. What would be the expected cost of predicting class 2?\n\n\n\n\n\n\n\nTable 2: Example of a cost matrix\n\n\n\n\n\n\n\nPredicted Class 1\nPredicted Class 2\n\n\n\n\nTrue Class 1\n-1\n4\n\n\nTrue Class 2\n2\n-2\n\n\n\n\n\n\n\n\n\n\n\nAnswer: 0.4\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: -0.4\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: 1.6\n\n\nCorrect. \\(4*0.6 - 2*0.4\\)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat would be the expected cost of predicting class 1?\n\n\n\n\n\n\nAnswer: 0.4\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: -0.4\n\n\nCorrect. \\(-1*0.6 + 2*0.4\\)\n\n\n\n\n\n\nAnswer: 1.6\n\n\nIncorrect. Try another answer."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#common-sources-of-miscalibration-1",
    "href": "cha_wahcc/slides-cla-cal.html#common-sources-of-miscalibration-1",
    "title": "Classifier Calibration",
    "section": "Common sources of miscalibration",
    "text": "Common sources of miscalibration\n\nUnderconfidence:\n\na classifier thinks it’s worse at separating classes than it actually is.\n\nHence we need to pull predicted probabilities away from the centre.\n\n\nOverconfidence:\n\na classifier thinks it’s better at separating classes than it actually is.\n\nHence we need to push predicted probabilities toward the centre.\n\n\n\nA classifier can be overconfident for one class and underconfident for the other, in which case all predicted probabilities need to be increased or decreased."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#underconfidence-example",
    "href": "cha_wahcc/slides-cla-cal.html#underconfidence-example",
    "title": "Classifier Calibration",
    "section": "Underconfidence example",
    "text": "Underconfidence example\n\n\n\nUnderconfidence typically gives sigmoidal distortions.\nTo calibrate these means to pull predicted probabilities away from the centre.\n\n\n\nSource: (Niculescu-Mizil and Caruana 2005)"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#overconfidence-example",
    "href": "cha_wahcc/slides-cla-cal.html#overconfidence-example",
    "title": "Classifier Calibration",
    "section": "Overconfidence example",
    "text": "Overconfidence example\n\n\n\nOverconfidence is very common, and usually a consequence of over-counting evidence.\nHere, distortions are inverse-sigmoidal\nCalibrating these means to push predicted probabilities toward the centre.\n\n\n\nSource: (Niculescu-Mizil and Caruana 2005)"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#why-fitting-the-distortions-helps-with-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#why-fitting-the-distortions-helps-with-calibration",
    "title": "Classifier Calibration",
    "section": "Why fitting the distortions helps with calibration",
    "text": "Why fitting the distortions helps with calibration\n\n\nIn clockwise direction, the dotted arrows show:\n\nusing a point’s uncalibrated score on the \\(x\\)-axis as input to the calibration map,\nmapping the resulting output back to the diagonal, and\ncombine with the empirical probability of the point we started from.\n\nThe closer the original point is to the fitted calibration map, the closer the calibrated point (in red) will be to the diagonal."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#questions-and-answers-2",
    "href": "cha_wahcc/slides-cla-cal.html#questions-and-answers-2",
    "title": "Classifier Calibration",
    "section": "Questions and answers",
    "text": "Questions and answers\nQ&A 1\n\n\n\n\n\n\n\n\nQuestion\n\n\nThe following figures show the reliability diagram of several binary classifiers. Assuming that there are enough samples on each bin, indicate if the model seems calibrated, over-confident or under-confident.\n\n\n\n\n\n\nAnswer: Calibrated.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\nCorrect.\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 2\n\n\n\n\n\nAnswer: Calibrated.\n\n\nCorrect.\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 3\n\n\n\n\n\nAnswer: Calibrated.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 4\n\n\n\n\n\n\nQuestion\n\n\nCan a binary classifier show a calibrated reliability diagram with a number of equally distributed bins, and a non-calibrated one with a higher number of equally distributed bins?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nCorrect.\n\n\n\n\n\n\nAnswer: No.\n\n\nIncorrect. Try another answer."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#a-first-look-at-some-calibration-techniques-1",
    "href": "cha_wahcc/slides-cla-cal.html#a-first-look-at-some-calibration-techniques-1",
    "title": "Classifier Calibration",
    "section": "A first look at some calibration techniques",
    "text": "A first look at some calibration techniques\n\nParametric calibration involves modelling the score distributions within each class. \\\n\nPlatt scaling = Logistic calibration can be derived by assuming that the scores within both classes are normally distributed with the same variance (Platt 2000).\nBeta calibration employs Beta distributions instead, to deal with scores already on a \\([0,1]\\) scale (Kull, Silva Filho, and Flach 2017).\nDirichlet calibration for more than two classes (Kull et al. 2019).\n\nNon-parametric calibration often ignores scores and employs ranks instead. \\\n\nE.g., isotonic regression = pool adjacent violators = ROC convex hull (Zadrozny and Elkan 2001) (Fawcett and Niculescu-Mizil 2007)."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#platt-scaling",
    "href": "cha_wahcc/slides-cla-cal.html#platt-scaling",
    "title": "Classifier Calibration",
    "section": "Platt scaling",
    "text": "Platt scaling\n\n\\[\\begin{align*}\n    p(s; w, m) &= \\frac{1}{1+\\exp(-w(s-m))}\\\\\n    w &= (\\mu_{\\textit{pos}}-\\mu_{\\textit{neg}})/\\sigma^2,\n    m = (\\mu_{\\textit{pos}}+\\mu_{\\textit{neg}})/2\n\\end{align*}\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#beta-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#beta-calibration",
    "title": "Classifier Calibration",
    "section": "Beta calibration",
    "text": "Beta calibration\n\n\\[\\begin{align*}\n  p(s; a, b, c) &= \\frac{1}{1+\\exp(-a \\ln s - b \\ln (1-s) - c)} \\\\\n  a &= \\alpha_{\\textit{pos}}-\\alpha_{\\textit{neg}},\n  b = \\beta_{\\textit{neg}}-\\beta_{\\textit{pos}}\n\\end{align*}\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#isotonic-regression",
    "href": "cha_wahcc/slides-cla-cal.html#isotonic-regression",
    "title": "Classifier Calibration",
    "section": "Isotonic regression",
    "text": "Isotonic regression\n\n\n\n\n\n\n\nSource: Flach (2016)"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#questions-and-answers-3",
    "href": "cha_wahcc/slides-cla-cal.html#questions-and-answers-3",
    "title": "Classifier Calibration",
    "section": "Questions and answers",
    "text": "Questions and answers\n\n\n\n\n\n\nQuestion\n\n\nCan a binary classifier show a calibrated reliability diagram with a number of equally distributed bins, and a non-calibrated one with a higher number of equally distributed bins?\n\n\n\n\n\n\nAnswer: What is a calibration map?\n\n\nIt is a mapping between the scores to be calibrated and the objective probabilities.\n\n\n\n\n\n\n\n\n\nPlatt scaling:\n\n\nCan Platt scaling calibrate probabilistic models that are overconfident?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect. Platt scaling for probability scores can only generate S shaped calibration maps, which can only calibrate under-confident scores.\n\n\n\n\n\n\n\n\n\nIsotonic regression:\n\n\nIs isotonic regression a parametric method?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nPlatt scaling:\n\n\nCan Platt scaling learn an identity function if the model is already calibrated?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect. Platt scaling can only learn S shaped functions, and the identity function requires a straight line."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#whats-so-special-about-multi-class-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#whats-so-special-about-multi-class-calibration",
    "title": "Classifier Calibration",
    "section": "What’s so special about multi-class calibration?",
    "text": "What’s so special about multi-class calibration?\nSimilar to classification, some methods are inherently multi-class but most are not.\nThis leads to (at least) three different ways of defining what it means to be fully multiclass-calibrated. - Many recent papers use the (weak) notion of confidence calibration.\nEvaluating multi-class calibration is in its full generality still an open problem."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#definitions-of-calibration-for-more-than-two-classes",
    "href": "cha_wahcc/slides-cla-cal.html#definitions-of-calibration-for-more-than-two-classes",
    "title": "Classifier Calibration",
    "section": "Definitions of calibration for more than two classes",
    "text": "Definitions of calibration for more than two classes\nThe following definitions of calibration are equivalent for binary classification but increasingly stronger for more than two classes:\n\nConfidence calibration: only consider the highest predicted probability.\nClass-wise calibration: only consider marginal probabilities.\nMulti-class calibration: consider the entire vector of predicted probabilities."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#confidence-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#confidence-calibration",
    "title": "Classifier Calibration",
    "section": "Confidence calibration",
    "text": "Confidence calibration\nThis was proposed by Guo et al. (2017), requiring that among all instances where the probability of the most likely class is predicted to be \\(c\\), the expected accuracy is \\(c\\). (We call this `confidence calibration’ to distinguish it from the stronger notions of calibration.)\nFormally, a probabilistic classifier \\(\\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k}\\) is confidence-calibrated, if for any confidence level \\(c\\in[0,1]\\), the actual proportion of the predicted class, among all possible instances \\(\\mathbf{x}\\) being predicted this class with confidence \\(c\\), is equal to \\(c\\):\n\\[\\begin{align*}\nP(Y=i \\: | \\: \\hat{p}_i(\\mathbf{x})=c)=c\\qquad\\text{where }\\ i=\\mathop{\\mathrm{arg\\,max}}_j \\hat{p}_j(\\mathbf{x}).\n%P\\Big(Y=\\argmax\\big(\\vph(X)\\big) \\: \\Big| \\: \\max\\big(\\vph(X)\\big)=c\\Big)=c.\n\\end{align*}\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#class-wise-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#class-wise-calibration",
    "title": "Classifier Calibration",
    "section": "Class-wise calibration",
    "text": "Class-wise calibration\nOriginally proposed by Zadrozny and Elkan (2002), this requires that all one-vs-rest probability estimators obtained from the original multiclass model are calibrated.\nFormally, a probabilistic classifier \\(\\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k}\\) is classwise-calibrated, if for any class \\(i\\) and any predicted probability \\(q_i\\) for this class, the actual proportion of class \\(i\\), among all possible instances \\(\\mathbf{x}\\) getting the same prediction \\(\\hat{p}_i(\\mathbf{x})=q_i\\), is equal to \\(q_i\\):\n\\[\\begin{align*}\nP(Y=i\\mid \\hat{p}_i(\\mathbf{x})=q_i)=q_i\\qquad\\text{for }\\ i=1,\\dots,k.\n\\end{align*}\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#multi-class-calibration",
    "href": "cha_wahcc/slides-cla-cal.html#multi-class-calibration",
    "title": "Classifier Calibration",
    "section": "Multi-class calibration",
    "text": "Multi-class calibration\nThis is the strongest form of calibration for multiple classes, subsuming the previous two definitions.\nA probabilistic classifier \\(\\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k}\\) is multiclass-calibrated if for any prediction vector \\(\\mathbf{q}=(q_1,\\dots,q_k)\\in\\Delta_{k}\\), the proportions of classes among all possible instances \\(\\mathbf{x}\\) getting the same prediction \\(\\mathbf{\\hat{p}}(\\mathbf{x})=\\mathbf{q}\\) are equal to the prediction vector \\(\\mathbf{q}\\):\n\\[\\begin{align*} %\\label{eq:calib}\nP(Y=i\\mid \\mathbf{\\hat{p}}(\\mathbf{x})=\\mathbf{q})=q_i\\qquad\\text{for }\\ i=1,\\dots,k.\n\\end{align*}\\]"
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#reminder-binning-needed",
    "href": "cha_wahcc/slides-cla-cal.html#reminder-binning-needed",
    "title": "Classifier Calibration",
    "section": "Reminder: binning needed",
    "text": "Reminder: binning needed\nFor practical purposes, the conditions in these definitions need to be relaxed. This is where binning comes in.\nOnce we have the bins, we can draw a reliability diagram as in the two-class case. For class-wise calibration, we can show per-class reliability diagrams or a single averaged one.\nThe degree of calibration is assessed using the gaps in the reliability diagram. All of this will be elaborated in the next part of the tutorial."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#important-points-to-remember",
    "href": "cha_wahcc/slides-cla-cal.html#important-points-to-remember",
    "title": "Classifier Calibration",
    "section": "Important points to remember",
    "text": "Important points to remember\n\nOnly well-calibrated probability estimates are worthy to be called probabilities: otherwise they are just scores that happen to be in the \\([0,1]\\) range.\nBinning will be required in some form: instance-based probability evaluation metrics such as Brier score or log-loss always measure calibration plus something else.\nIn multi-class settings, think carefully about which form of calibration you need: e.g., confidence-calibration is too weak in a cost-sensitive setting."
  },
  {
    "objectID": "cha_wahcc/slides-cla-cal.html#questions-and-answers-4",
    "href": "cha_wahcc/slides-cla-cal.html#questions-and-answers-4",
    "title": "Classifier Calibration",
    "section": "Questions and answers",
    "text": "Questions and answers\n\n\n\n\n\n\nModel scores:\n\n\nCan we interpret the output of any model that produces values between zero and one as probabilities?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect. Some models have been trained to generate values in any arbitrary range, but this does not mean that the model is predicting actual probabilities.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nIs there only one way to measure calibration for multiclass probability scores.\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect. There are multiple measures of calibration.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nCan we perform optimal decisions in a multiclass setting by knowing the highest probability among the classes and the misclassification costs?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect. We need the probabilities of every class in order to make an optimal decision.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nIf the data distribution, operating conditions and the missclassification costs do not change from training to test set, and a model makes optimal predictions in the training set. Do we need the exact probabilities in the test set to make optimal decisions?\n\n\n\n\n\n\nAnswer: Yes.\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nAnswer: No.\n\n\nCorrect."
  },
  {
    "objectID": "cha_wahcc/wahcc.html",
    "href": "cha_wahcc/wahcc.html",
    "title": "Classifier Calibration",
    "section": "",
    "text": "Weather forecasters started thinking about calibration a long time ago (Brier 1950).\n\nA forecast 70% chance of rain should be followed by rain 70% of the time.\n\nThis is immediately applicable to binary classification:\n\nA prediction 70% chance of spam should be spam 70% of the time.\n\nand to multi-class classification:\n\nA prediction 70% chance of setosa, 10% chance of versicolor and 20% chance of virginica should be setosa/versicolor/virginica 70/10/20% of the time.\n\nIn general:\n\nA predicted probability (vector) should match empirical (observed) probabilities.\n\n\n\n\n\n\n\n\nQ: What does X% of the time mean?\n\n\n\n\n\nIt means that we expect the occurrence of an event to happen “X%” of the time.\n\n\n\n\n\n\nLet’s consider a small toy example:\n\nTwo predictions of 10% chance of rain were both followed by no rain.\nTwo predictions of 40% chance of rain were once followed by no rain, and once by rain.\nThree predictions of 70% chance of rain were once followed by no rain, and twice by rain.\nOne prediction of 90% chance of rain was followed by rain.\n\n\n\n\n\n\n\nQ: Is this forecaster well-calibrated?\n\n\n\n\n\nThe evaluation of calibration requires a large number of samples to make a statement. However, in this toy example we can assume that a 10\\% discrepancy is acceptable, and that the number of samples is sufficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\nThis forecaster is doing a pretty decent job:\n\n10% chance of rain was a slight over-estimate\n(\\bar{y} = 0/2 =   0\\%);\n40% chance of rain was a slight under-estimate\n(\\bar{y} = 1/2 =  50\\%);\n70% chance of rain was a slight over-estimate\n(\\bar{y} = 2/3 =  67\\%);\n90% chance of rain was a slight under-estimate\n(\\bar{y} = 1/1 = 100\\%).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.1 ,0.4, 0.4,0.7, 0.7, 0.7, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.2\n0\n0\n\n\n2\n3\n0.3\n0.4\n0\n1\n\n\n4\n5\n6\n0.6\n0.7\n0.8\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n2\n3\n0.1\n0.2\n0.3\n0.4\n0\n0\n0\n1\n\n\n4\n5\n6\n7\n0.6\n0.7\n0.8\n0.9\n0\n1\n1\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.5, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n0.1\n0\n\n\n1\n0.2\n0\n\n\n2\n0.3\n0\n\n\n3\n0.4\n1\n\n\n4\n0.6\n0\n\n\n5\n0.7\n1\n\n\n6\n0.8\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.101, 0.201, 0.301, 0.401, 0.601, 0.701, 0.801, 0.901, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need bins to evaluate the degree of calibration:\n\nIn order to decide whether a weather forecaster is well-calibrated, we need to look at a good number of forecasts, say over one year.\nWe also need to make sure that there are a reasonable number of forecasts for separate probability values, so we can obtain reliable empirical estimates.\n\nTrade-off: large bins give better empirical estimates, small bins allows a more fine-grained assessment of calibration.}\n\n\nBut adjusting forecasts in groups also gives rise to practical calibration methods:\n\nempirical binning\nisotonic regression (aka ROC convex hull)\n\n\n\n\nIn the following example you can change the number of bins of a fixed set of samples and see how the Expected Calibration Error changes accordingly.\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nfrom utils import plot_reliability_diagram\nimport numpy as np\n\nnp.random.seed(42)\nscores = np.random.rand(100)\nlabels = np.random.binomial(1, scores)\n\nimport numpy as np\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n    ui.input_slider(\"n\", \"Number of bins\", \n                    min=1, max=100, value=10),\n    ),\n    ui.panel_main(\n    ui.output_plot(\"plot\")\n    )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        fig = plot_reliability_diagram(labels, scores,\n                                       n_bins=input.n())\n        return fig\n\napp = App(app_ui, server, debug=True)\n\n\n## file: utils.py\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\nimport matplotlib.ticker as mticker\n\ndef get_binned_scores(labels, scores, bins=10):\n    '''\n    Parameters\n    ==========\n    labels : array (n_samples, )\n        Labels indicating the true class.\n    scores : matrix (n_samples, )\n        Output probability scores for one or several methods.\n    bins : int or list of floats\n        Number of bins to create in the scores' space, or list of bin\n        boundaries.\n    '''\n    n_bins = 1\n    if isinstance(bins, int):\n        n_bins = bins\n        bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n    elif isinstance(bins, list) or isinstance(bins, np.ndarray):\n        n_bins = len(bins) - 1\n        bins = np.array(bins)\n        if bins[0] == 0.0:\n            bins[0] = 0 - 1e-8\n        if bins[-1] == 1.0:\n            bins[-1] = 1 + 1e-8\n\n    scores = np.clip(scores, a_min=0, a_max=1)\n\n    bin_idx = np.digitize(scores, bins) - 1\n\n    bin_true = np.bincount(bin_idx, weights=labels,\n                           minlength=n_bins)\n    bin_pred = np.bincount(bin_idx, weights=scores,\n                           minlength=n_bins)\n    bin_total = np.bincount(bin_idx, minlength=n_bins)\n\n    zero_idx = bin_total == 0\n    avg_true = np.empty(bin_total.shape[0])\n    avg_true.fill(np.nan)\n    avg_true[~zero_idx] = np.divide(bin_true[~zero_idx],\n                                    bin_total[~zero_idx])\n    avg_pred = np.empty(bin_total.shape[0])\n    avg_pred.fill(np.nan)\n    avg_pred[~zero_idx] = np.divide(bin_pred[~zero_idx],\n                                    bin_total[~zero_idx])\n    return avg_true, avg_pred, bin_true, bin_total\n    \ndef plot_reliability_diagram(labels, scores, n_bins=10):\n    bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n\n    avg_true, avg_pred, bin_true, bin_total = get_binned_scores(\n            labels, scores, bins=bins)\n\n    zero_idx = bin_total == 0\n    fig = plt.figure()\n    ax1 = fig.add_subplot()\n    fig, axs = plt.subplots(2, 1,\n                            gridspec_kw={'height_ratios': [4, 1]})\n    axs[0].bar(x=bins[:-1][~zero_idx], height=avg_true[~zero_idx],\n                        align='edge', width=(bins[1:] - bins[:-1])[~zero_idx],\n                        edgecolor='black')\n    axs[0].scatter(avg_pred, avg_true)\n    axs[0].plot([0, 1], [0, 1], '--', color='red')\n\n    axs[0].set_xlim([0, 1])\n    axs[0].set_ylim([0, 1])\n    axs[0].set_ylabel('Fraction of positives')\n    axs[0].grid(True)\n    axs[0].set_axisbelow(True)\n\n    axs[1].hist(scores, range=(0, 1),\n             bins=bins,\n             histtype=\"bar\",\n             lw=1,\n             edgecolor='black')\n\n    axs[1].set_xlim([0, 1])\n    axs[1].set_ylabel('Count')\n    axs[1].set_xlabel('Scores')\n\n\n    weights = bin_total[~zero_idx]/sum(bin_total)\n    ece = sum(np.abs(avg_pred - avg_true)[~zero_idx]*weights)\n    fig.suptitle(f'Expected Calibration Error = {ece:0.3f}')\n    return fig\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times but it does not rain, two times 0.4 and it rains only once, five times 0.6 and it rains 80% of the times and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nCorrect. You can see that there is one bin per predicted score 0.1, 0.4, 0.6 and 0.9. Each bin contains the number of scores indicated in the smaller histogram below with 2, 2, 5 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 0%, 50%, 80% and 100%.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0,   0,  0,  1,  0,  1,  1,  1,  1,  1])\nscores = np.array([.1, .1, .4, .4, .6, .6, .6, .6, .6, .9])\nfig = plt.figure(figsize=(5, 3))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=4,\n                               fig=fig,\n                               hist_per_class=False,\n                               show_bars=True,\n                               show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times and it rains once, three times 0.4 and it rains two times, four times 0.6 and it rains once and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nCorrect. You can see that there is one bin per predicted score 0.1, 0.4, 0.6 and 0.9. Each bin contains the number of scores indicated in the smaller histogram below with 2, 3, 4 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 50%, 66.6%, 25% and 100%.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nlabels = np.array([0,   1,  0,  1,  1,  0,  0,  0,  1,  1])\nscores = np.array([.1, .1, .4, .4, .4, .6, .6, .6, .6, .9])\nfig = plt.figure(figsize=(5, 3))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=4,\n                               fig=fig,\n                               hist_per_class=False,\n                               show_bars=False,\n                               show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nDo we need multiple instances in each bin in order to visualise a reliability diagram?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nCorrect. It is not necessary to have multiple instances in each bin for visualisation purposes. However, the lack of information does not allow us to know if the model is calibrated for those scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\n\n\n\nCorrect. For each predicted score the actual fraction of positives is higher.\n\n\n\n\n\n\nShow the code\nimport numpy as np\n\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores = np.array([0, .1, .3, .6, .8, 1]).reshape(-1, 1)\nempirical = np.array([.1, .2, .5, .7, .9, 1]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\n\n\n\nCorrect. For each predicted score the actual fraction of positives is lower.\n\n\n\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\n\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores = np.array([0, .1, .3, .5, .7, .8, .9, 1]).reshape(-1, 1)\nempirical = np.array([0, .05, .1, .3, .33, .6, .8, 1]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "cha_wahcc/wahcc.html#taking-inspiration-from-forecasting",
    "href": "cha_wahcc/wahcc.html#taking-inspiration-from-forecasting",
    "title": "Classifier Calibration",
    "section": "",
    "text": "Weather forecasters started thinking about calibration a long time ago (Brier 1950).\n\nA forecast 70% chance of rain should be followed by rain 70% of the time.\n\nThis is immediately applicable to binary classification:\n\nA prediction 70% chance of spam should be spam 70% of the time.\n\nand to multi-class classification:\n\nA prediction 70% chance of setosa, 10% chance of versicolor and 20% chance of virginica should be setosa/versicolor/virginica 70/10/20% of the time.\n\nIn general:\n\nA predicted probability (vector) should match empirical (observed) probabilities.\n\n\n\n\n\n\n\n\nQ: What does X% of the time mean?\n\n\n\n\n\nIt means that we expect the occurrence of an event to happen “X%” of the time.\n\n\n\n\n\n\nLet’s consider a small toy example:\n\nTwo predictions of 10% chance of rain were both followed by no rain.\nTwo predictions of 40% chance of rain were once followed by no rain, and once by rain.\nThree predictions of 70% chance of rain were once followed by no rain, and twice by rain.\nOne prediction of 90% chance of rain was followed by rain.\n\n\n\n\n\n\n\nQ: Is this forecaster well-calibrated?\n\n\n\n\n\nThe evaluation of calibration requires a large number of samples to make a statement. However, in this toy example we can assume that a 10\\% discrepancy is acceptable, and that the number of samples is sufficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\nThis forecaster is doing a pretty decent job:\n\n10% chance of rain was a slight over-estimate\n(\\bar{y} = 0/2 =   0\\%);\n40% chance of rain was a slight under-estimate\n(\\bar{y} = 1/2 =  50\\%);\n70% chance of rain was a slight over-estimate\n(\\bar{y} = 2/3 =  67\\%);\n90% chance of rain was a slight under-estimate\n(\\bar{y} = 1/1 = 100\\%).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.1 ,0.4, 0.4,0.7, 0.7, 0.7, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n0.1\n0.2\n0\n0\n\n\n2\n3\n0.3\n0.4\n0\n1\n\n\n4\n5\n6\n0.6\n0.7\n0.8\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n1\n2\n3\n0.1\n0.2\n0.3\n0.4\n0\n0\n0\n1\n\n\n4\n5\n6\n7\n0.6\n0.7\n0.8\n0.9\n0\n1\n1\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.5, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\hat{p}\ny\n\n\n\n\n0\n0.1\n0\n\n\n1\n0.2\n0\n\n\n2\n0.3\n0\n\n\n3\n0.4\n1\n\n\n4\n0.6\n0\n\n\n5\n0.7\n1\n\n\n6\n0.8\n1\n\n\n7\n0.9\n1\n\n\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.101, 0.201, 0.301, 0.401, 0.601, 0.701, 0.801, 0.901, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need bins to evaluate the degree of calibration:\n\nIn order to decide whether a weather forecaster is well-calibrated, we need to look at a good number of forecasts, say over one year.\nWe also need to make sure that there are a reasonable number of forecasts for separate probability values, so we can obtain reliable empirical estimates.\n\nTrade-off: large bins give better empirical estimates, small bins allows a more fine-grained assessment of calibration.}\n\n\nBut adjusting forecasts in groups also gives rise to practical calibration methods:\n\nempirical binning\nisotonic regression (aka ROC convex hull)\n\n\n\n\nIn the following example you can change the number of bins of a fixed set of samples and see how the Expected Calibration Error changes accordingly.\n#| standalone: true\n#| components: viewer\n#| viewerHeight: 480\n\nfrom utils import plot_reliability_diagram\nimport numpy as np\n\nnp.random.seed(42)\nscores = np.random.rand(100)\nlabels = np.random.binomial(1, scores)\n\nimport numpy as np\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n    ui.input_slider(\"n\", \"Number of bins\", \n                    min=1, max=100, value=10),\n    ),\n    ui.panel_main(\n    ui.output_plot(\"plot\")\n    )\n    ),\n)\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"A histogram\")\n    def plot():\n        fig = plot_reliability_diagram(labels, scores,\n                                       n_bins=input.n())\n        return fig\n\napp = App(app_ui, server, debug=True)\n\n\n## file: utils.py\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\nimport matplotlib.ticker as mticker\n\ndef get_binned_scores(labels, scores, bins=10):\n    '''\n    Parameters\n    ==========\n    labels : array (n_samples, )\n        Labels indicating the true class.\n    scores : matrix (n_samples, )\n        Output probability scores for one or several methods.\n    bins : int or list of floats\n        Number of bins to create in the scores' space, or list of bin\n        boundaries.\n    '''\n    n_bins = 1\n    if isinstance(bins, int):\n        n_bins = bins\n        bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n    elif isinstance(bins, list) or isinstance(bins, np.ndarray):\n        n_bins = len(bins) - 1\n        bins = np.array(bins)\n        if bins[0] == 0.0:\n            bins[0] = 0 - 1e-8\n        if bins[-1] == 1.0:\n            bins[-1] = 1 + 1e-8\n\n    scores = np.clip(scores, a_min=0, a_max=1)\n\n    bin_idx = np.digitize(scores, bins) - 1\n\n    bin_true = np.bincount(bin_idx, weights=labels,\n                           minlength=n_bins)\n    bin_pred = np.bincount(bin_idx, weights=scores,\n                           minlength=n_bins)\n    bin_total = np.bincount(bin_idx, minlength=n_bins)\n\n    zero_idx = bin_total == 0\n    avg_true = np.empty(bin_total.shape[0])\n    avg_true.fill(np.nan)\n    avg_true[~zero_idx] = np.divide(bin_true[~zero_idx],\n                                    bin_total[~zero_idx])\n    avg_pred = np.empty(bin_total.shape[0])\n    avg_pred.fill(np.nan)\n    avg_pred[~zero_idx] = np.divide(bin_pred[~zero_idx],\n                                    bin_total[~zero_idx])\n    return avg_true, avg_pred, bin_true, bin_total\n    \ndef plot_reliability_diagram(labels, scores, n_bins=10):\n    bins = np.linspace(0, 1 + 1e-8, n_bins + 1)\n\n    avg_true, avg_pred, bin_true, bin_total = get_binned_scores(\n            labels, scores, bins=bins)\n\n    zero_idx = bin_total == 0\n    fig = plt.figure()\n    ax1 = fig.add_subplot()\n    fig, axs = plt.subplots(2, 1,\n                            gridspec_kw={'height_ratios': [4, 1]})\n    axs[0].bar(x=bins[:-1][~zero_idx], height=avg_true[~zero_idx],\n                        align='edge', width=(bins[1:] - bins[:-1])[~zero_idx],\n                        edgecolor='black')\n    axs[0].scatter(avg_pred, avg_true)\n    axs[0].plot([0, 1], [0, 1], '--', color='red')\n\n    axs[0].set_xlim([0, 1])\n    axs[0].set_ylim([0, 1])\n    axs[0].set_ylabel('Fraction of positives')\n    axs[0].grid(True)\n    axs[0].set_axisbelow(True)\n\n    axs[1].hist(scores, range=(0, 1),\n             bins=bins,\n             histtype=\"bar\",\n             lw=1,\n             edgecolor='black')\n\n    axs[1].set_xlim([0, 1])\n    axs[1].set_ylabel('Count')\n    axs[1].set_xlabel('Scores')\n\n\n    weights = bin_total[~zero_idx]/sum(bin_total)\n    ece = sum(np.abs(avg_pred - avg_true)[~zero_idx]*weights)\n    fig.suptitle(f'Expected Calibration Error = {ece:0.3f}')\n    return fig\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times but it does not rain, two times 0.4 and it rains only once, five times 0.6 and it rains 80% of the times and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nCorrect. You can see that there is one bin per predicted score 0.1, 0.4, 0.6 and 0.9. Each bin contains the number of scores indicated in the smaller histogram below with 2, 2, 5 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 0%, 50%, 80% and 100%.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0,   0,  0,  1,  0,  1,  1,  1,  1,  1])\nscores = np.array([.1, .1, .4, .4, .6, .6, .6, .6, .6, .9])\nfig = plt.figure(figsize=(5, 3))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=4,\n                               fig=fig,\n                               hist_per_class=False,\n                               show_bars=True,\n                               show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nA binary classifier for weather predictions produces a score of 0.1 for rain two times and it rains once, three times 0.4 and it rains two times, four times 0.6 and it rains once and one time 0.9 and it rains. Does the following reliability diagram show that information?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nCorrect. You can see that there is one bin per predicted score 0.1, 0.4, 0.6 and 0.9. Each bin contains the number of scores indicated in the smaller histogram below with 2, 3, 4 and 1 samples respectively. Finally, the height of each bin corresponds to the fraction of rains indicated in the question 50%, 66.6%, 25% and 100%.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nlabels = np.array([0,   1,  0,  1,  1,  0,  0,  0,  1,  1])\nscores = np.array([.1, .1, .4, .4, .4, .6, .6, .6, .6, .9])\nfig = plt.figure(figsize=(5, 3))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=4,\n                               fig=fig,\n                               hist_per_class=False,\n                               show_bars=False,\n                               show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nDo we need multiple instances in each bin in order to visualise a reliability diagram?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nCorrect. It is not necessary to have multiple instances in each bin for visualisation purposes. However, the lack of information does not allow us to know if the model is calibrated for those scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\n\n\n\nCorrect. For each predicted score the actual fraction of positives is higher.\n\n\n\n\n\n\nShow the code\nimport numpy as np\n\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores = np.array([0, .1, .3, .6, .8, 1]).reshape(-1, 1)\nempirical = np.array([.1, .2, .5, .7, .9, 1]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nThe following figure shows the reliability diagram of a binary classifier on enough samples to be statistically significant. Is the model calibrated, producing under-estimates or over-estimates?\n\n\n\n\n\n\n\n\nAnswer: Over-estimates\n\n\n\n\n\nCorrect. For each predicted score the actual fraction of positives is lower.\n\n\n\n\n\n\n\n\n\nAnswer: Under-estimates\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\n\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores = np.array([0, .1, .3, .5, .7, .8, .9, 1]).reshape(-1, 1)\nempirical = np.array([0, .05, .1, .3, .33, .6, .8, 1]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "cha_wahcc/wahcc.html#why-are-we-interested-in-calibration",
    "href": "cha_wahcc/wahcc.html#why-are-we-interested-in-calibration",
    "title": "Classifier Calibration",
    "section": "Why are we interested in calibration?",
    "text": "Why are we interested in calibration?\n\n\n\nWhy are we interested in calibration?\nTo calibrate means to employ a known scale with known properties.\n\nE.g., additive scale with a well-defined zero, so that ratios are meaningful.\n\nFor classifiers we want to use the probability scale, so that we can\n\njustifiably use default decision rules (e.g., maximum posterior probability);\nadjust these decision rules in a straightforward way to account for different class priors or misclassification costs;\ncombine probability estimates in a well-founded way.\n\n\n\n\n\n\n\nQ: Is the probability scale additive?\n\n\n\n\n\nIn some situations we may want to sum probabilities, for example if we have a set of mutually exclusive events, the probability of at least one of them happening can be computed by their sum. In other situations the product of probabilities is used, e.g. the probability of two independent events happening at the same time.\n\n\n\n\n\n\n\n\n\nQ: How would you combine probability estimates from several well-calibrated models?\n\n\n\n\n\nCheck some online information e.g. When pooling forecasts, use the geometric mean of odds\nAnd the following code shows some examples.\n\n\n\n\n\nShow the code\nimport numpy as np\nfrom tabulate import tabulate\nfrom IPython.display import Markdown\n\ndef mean(p):\n    '''Arithmetic mean'''\n    p = np.array(p)\n    return np.sum(p)/(len(p))\n\ndef gmean(p):\n    '''Geometric mean'''\n    p = np.array(p)\n    o = np.power(np.prod(p/(1-p)), 1/len(p))\n    return o/(1+o)\n\ndef hmean(p):\n    '''Harmonic mean'''\n    p = np.array(p)\n    return len(p)/np.sum(1/p)\n\nexample_list = [[.1, .1], [.5, .5], [.1, .9],\n                [.1, .1, .9], [.1, .1, .99], [.1, .1, .999]]\n\nfunctions = {'Arithmetic mean': mean,\n             'Geometric mean': gmean,\n             'Harmonic mean': hmean}\n\ntable = []\n\nfor example in example_list:\n    table.append([np.array2string(np.array(example))])\n    table[-1].extend([f'{f(example):.2f}' for f in functions.values()])\n\n\nheaders = ['Probabilities']\nheaders.extend(list(functions.keys()))\n\nMarkdown(tabulate(table, headers=headers))\n\n\n\n\n\n\n\n\nProbabilities\nArithmetic mean\nGeometric mean\nHarmonic mean\n\n\n\n\n[0.1 0.1]\n0.1\n0.1\n0.1\n\n\n[0.5 0.5]\n0.5\n0.5\n0.5\n\n\n[0.1 0.9]\n0.5\n0.5\n0.18\n\n\n[0.1 0.1 0.9]\n0.37\n0.32\n0.14\n\n\n[0.1 0.1 0.99]\n0.4\n0.52\n0.14\n\n\n[0.1 0.1 0.999]\n0.4\n0.7\n0.14\n\n\n\n\n\n\nTable 1: Example of probability means\n\n\n\n\n\n\nOptimal decisions I\nDenote the cost of predicting class j for an instance of true class i as C(\\hat{Y}=j|Y=i). The expected cost of predicting class j for instance x is\n\nC(\\hat{Y}=j|X=x) = \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\nwhere P(Y=i|X=x) is the probability of instance x having true class i (as would be given by the Bayes-optimal classifier).\nThe optimal decision is then to predict the class with lowest expected cost:\n\n\\hat{Y}^* = \\mathop{\\mathrm{arg\\,min}}_j C(\\hat{Y}=j|X=x) = \\mathop{\\mathrm{arg\\,min}}_j \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\n\n\nOptimal decisions II\nIn binary classification we have:\n\\begin{align*}\nC(\\hat{Y}=+|X=x) &= P(+|x)C(+|+) + \\big(1-P(+|x)\\big)C(+|-) \\\\\nC(\\hat{Y}=-|X=x) &= P(+|x)C(-|+) + \\big(1-P(+|x)\\big)C(-|-)\n\\end{align*}\nOn the optimal decision boundary these two expected costs are equal, which gives\n\\begin{align*}%\\label{eq::cost-threshold}\nP(+|x)  = \\frac{\\color{blue}{C(+|-)-C(-|-)}}{\\color{blue}{C(+|-)-C(-|-)}+\\color{red}{C(-|+)-C(+|+)}} \\triangleq c\n\\end{align*}\nThis gives the optimal threshold on the hypothetical Bayes-optimal probabilities.\nIt is also the best thing to do in practice – as long as the probabilities are well-calibrated!\n\n\nOptimal decisions III\nWithout loss of generality we can set the cost of true positives and true negatives to zero; c = \\frac{c_{\\text{FP}}}{c_{\\text{FP}} + c_{\\text{FN}}} is then the cost of a false positive in proportion to the combined cost of one false positive and one false negative.\n\nE.g., if false positives are 4 times as costly as false negatives then we set the decision threshold to 4/(4+1)=0.8 in order to only make positive predictions if we’re pretty certain.\n\nSimilar reasoning applies to changes in class priors:\n\nif we trained on balanced classes but want to deploy with 4 times as many positives compared to negatives, we lower the decision threshold to 0.2;\nmore generally, if we trained for class ratio r and deploy for class ratio r' we set the decision threshold to r/(r+r').\n\nCost and class prior changes can be combined in the obvious way.\n\n\nQuestions and answers\n\nQ&A 1\n\n\n\n\n\n\nQuestion 1\n\n\n\nIs it possible to compute optimal risks given a cost matrix and a probabilistic classifier that is not calibrated?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nCorrect.\n\n\n\n\n\nQ&A 2\n\n\n\n\n\n\nQuestion\n\n\n\nGiven a calibrated probabilistic classifier, is it optimal to select the class with the highest predicted probability?\n\n\n\n\n\n\n\n\nAnswer: Yes\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf we have the following cost matrix, and a model outputs 0.6 probability for class 1. What would be the expected cost of predicting class 2?\n\n\n\n\nShow the code\nimport numpy as np\nfrom tabulate import tabulate\nfrom IPython.display import Markdown\n\ncost_matrix = [[-1,  4],\n               [ 2, -2]]\n\ntable = [['True Class 1'],\n         ['True Class 2']]\n\nfor i, c in enumerate(cost_matrix):\n    table[i].extend(c)\n\nheaders = ['Predicted Class 1', 'Predicted Class 2']\n\nMarkdown(tabulate(table, headers=headers))\n\n\n\n\n\n\n\n\n\nPredicted Class 1\nPredicted Class 2\n\n\n\n\nTrue Class 1\n-1\n4\n\n\nTrue Class 2\n2\n-2\n\n\n\n\n\n\nTable 2: Example of a cost matrix\n\n\n\n\n\n\n\n\n\n\nAnswer: 0.4\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: -0.4\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: 1.6\n\n\n\n\n\nCorrect. 4*0.6 - 2*0.4\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat would be the expected cost of predicting class 1?\n\n\n\n\n\n\n\n\nAnswer: 0.4\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: -0.4\n\n\n\n\n\nCorrect. -1*0.6 + 2*0.4\n\n\n\n\n\n\n\n\n\nAnswer: 1.6\n\n\n\n\n\nIncorrect. Try another answer.",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "cha_wahcc/wahcc.html#common-sources-of-miscalibration",
    "href": "cha_wahcc/wahcc.html#common-sources-of-miscalibration",
    "title": "Classifier Calibration",
    "section": "Common sources of miscalibration",
    "text": "Common sources of miscalibration\n\n\n\nCommon sources of miscalibration\n\nUnderconfidence:\n\na classifier thinks it’s worse at separating classes than it actually is.\n\nHence we need to pull predicted probabilities away from the centre.\n\n\nOverconfidence:\n\na classifier thinks it’s better at separating classes than it actually is.\n\nHence we need to push predicted probabilities toward the centre.\n\n\n\nA classifier can be overconfident for one class and underconfident for the other, in which case all predicted probabilities need to be increased or decreased.\n\n\nUnderconfidence example\n\n\n\nUnderconfidence typically gives sigmoidal distortions.\nTo calibrate these means to pull predicted probabilities away from the centre.\n\n\n\nSource: (Niculescu-Mizil and Caruana 2005)\n\n\n\n\nOverconfidence example\n\n\n\nOverconfidence is very common, and usually a consequence of over-counting evidence.\nHere, distortions are inverse-sigmoidal\nCalibrating these means to push predicted probabilities toward the centre.\n\n\n\nSource: (Niculescu-Mizil and Caruana 2005)\n\n\n\n\nWhy fitting the distortions helps with calibration\n\n\nIn clockwise direction, the dotted arrows show:\n\nusing a point’s uncalibrated score on the x-axis as input to the calibration map,\nmapping the resulting output back to the diagonal, and\ncombine with the empirical probability of the point we started from.\n\nThe closer the original point is to the fitted calibration map, the closer the calibrated point (in red) will be to the diagonal.\n\n\n\n\n\n\nQuestions and answers\n\nQ&A 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nThe following figures show the reliability diagram of several binary classifiers. Assuming that there are enough samples on each bin, indicate if the model seems calibrated, over-confident or under-confident.\n\n\n\n\n\n\n\n\nAnswer: Calibrated.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores =    np.array([0., .1, .3, .4, .7, .9, 1.]).reshape(-1, 1)\nempirical = np.array([.1, .2, .4, .5, .6, .8, .9]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 2\n\n\n\n\n\n\n\n\nAnswer: Calibrated.\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\nShow the code\nimport numpy as np\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores = np.array([0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1]).reshape(-1, 1)\nempirical = scores\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 3\n\n\n\n\n\n\n\n\nAnswer: Calibrated.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Over-confident.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: Under-confident.\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\nShow the code\nimport numpy as np\nfrom pycalib.visualisations import plot_reliability_diagram_precomputed\n\nscores =    np.array([.1, .3, .5, .6, .8, .9]).reshape(-1, 1)\nempirical = np.array([0., .2, .5, .7, .9, 1.]).reshape(-1, 1)\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram_precomputed(avg_true=empirical, avg_pred=scores,\n                               class_names=['rain'],\n                               fig=fig, show_gaps=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A 4\n\n\n\n\n\n\nQuestion\n\n\n\nCan a binary classifier show a calibrated reliability diagram with a number of equally distributed bins, and a non-calibrated one with a higher number of equally distributed bins?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nIncorrect. Try another answer.",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "cha_wahcc/wahcc.html#a-first-look-at-some-calibration-techniques",
    "href": "cha_wahcc/wahcc.html#a-first-look-at-some-calibration-techniques",
    "title": "Classifier Calibration",
    "section": "A first look at some calibration techniques",
    "text": "A first look at some calibration techniques\n\n\n\nA first look at some calibration techniques\n\nParametric calibration involves modelling the score distributions within each class. \\\n\nPlatt scaling = Logistic calibration can be derived by assuming that the scores within both classes are normally distributed with the same variance (Platt 2000).\nBeta calibration employs Beta distributions instead, to deal with scores already on a [0,1] scale (Kull, Silva Filho, and Flach 2017).\nDirichlet calibration for more than two classes (Kull et al. 2019).\n\nNon-parametric calibration often ignores scores and employs ranks instead. \\\n\nE.g., isotonic regression = pool adjacent violators = ROC convex hull (Zadrozny and Elkan 2001) (Fawcett and Niculescu-Mizil 2007).\n\n\n\n\nPlatt scaling\n\n\\begin{align*}\n    p(s; w, m) &= \\frac{1}{1+\\exp(-w(s-m))}\\\\\n    w &= (\\mu_{\\textit{pos}}-\\mu_{\\textit{neg}})/\\sigma^2,\n    m = (\\mu_{\\textit{pos}}+\\mu_{\\textit{neg}})/2\n\\end{align*}\n\n\nBeta calibration\n\n\\begin{align*}\n  p(s; a, b, c) &= \\frac{1}{1+\\exp(-a \\ln s - b \\ln (1-s) - c)} \\\\\n  a &= \\alpha_{\\textit{pos}}-\\alpha_{\\textit{neg}},\n  b = \\beta_{\\textit{neg}}-\\beta_{\\textit{pos}}\n\\end{align*}\n\n\nIsotonic regression\n\n\n\n\n\n\n\nSource: Flach (2016)\n\n\n\n\nQuestions and answers\n\n\n\n\n\n\nQuestion\n\n\n\nCan a binary classifier show a calibrated reliability diagram with a number of equally distributed bins, and a non-calibrated one with a higher number of equally distributed bins?\n\n\n\n\n\n\n\n\nAnswer: What is a calibration map?\n\n\n\n\n\nIt is a mapping between the scores to be calibrated and the objective probabilities.\n\n\n\n\n\n\n\n\n\nPlatt scaling:\n\n\n\nCan Platt scaling calibrate probabilistic models that are overconfident?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect. Platt scaling for probability scores can only generate S shaped calibration maps, which can only calibrate under-confident scores.\n\n\n\n\n\n\n\n\n\nIsotonic regression:\n\n\n\nIs isotonic regression a parametric method?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect.\n\n\n\n\n\n\n\n\n\nPlatt scaling:\n\n\n\nCan Platt scaling learn an identity function if the model is already calibrated?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect. Platt scaling can only learn S shaped functions, and the identity function requires a straight line.",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "cha_wahcc/wahcc.html#calibrating-multi-class-classifiers",
    "href": "cha_wahcc/wahcc.html#calibrating-multi-class-classifiers",
    "title": "Classifier Calibration",
    "section": "Calibrating multi-class classifiers",
    "text": "Calibrating multi-class classifiers\n\n\n\nWhat’s so special about multi-class calibration?\nSimilar to classification, some methods are inherently multi-class but most are not.\nThis leads to (at least) three different ways of defining what it means to be fully multiclass-calibrated. - Many recent papers use the (weak) notion of confidence calibration.\nEvaluating multi-class calibration is in its full generality still an open problem.\n\n\nDefinitions of calibration for more than two classes\nThe following definitions of calibration are equivalent for binary classification but increasingly stronger for more than two classes:\n\nConfidence calibration: only consider the highest predicted probability.\nClass-wise calibration: only consider marginal probabilities.\nMulti-class calibration: consider the entire vector of predicted probabilities.\n\n\n\nConfidence calibration\nThis was proposed by Guo et al. (2017), requiring that among all instances where the probability of the most likely class is predicted to be c, the expected accuracy is c. (We call this `confidence calibration’ to distinguish it from the stronger notions of calibration.)\nFormally, a probabilistic classifier \\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k} is confidence-calibrated, if for any confidence level c\\in[0,1], the actual proportion of the predicted class, among all possible instances \\mathbf{x} being predicted this class with confidence c, is equal to c:\n\\begin{align*}\nP(Y=i \\: | \\: \\hat{p}_i(\\mathbf{x})=c)=c\\qquad\\text{where }\\ i=\\mathop{\\mathrm{arg\\,max}}_j \\hat{p}_j(\\mathbf{x}).\n%P\\Big(Y=\\argmax\\big(\\vph(X)\\big) \\: \\Big| \\: \\max\\big(\\vph(X)\\big)=c\\Big)=c.\n\\end{align*}\n\n\nClass-wise calibration\nOriginally proposed by Zadrozny and Elkan (2002), this requires that all one-vs-rest probability estimators obtained from the original multiclass model are calibrated.\nFormally, a probabilistic classifier \\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k} is classwise-calibrated, if for any class i and any predicted probability q_i for this class, the actual proportion of class i, among all possible instances \\mathbf{x} getting the same prediction \\hat{p}_i(\\mathbf{x})=q_i, is equal to q_i:\n\\begin{align*}\nP(Y=i\\mid \\hat{p}_i(\\mathbf{x})=q_i)=q_i\\qquad\\text{for }\\ i=1,\\dots,k.\n\\end{align*}\n\n\nMulti-class calibration\nThis is the strongest form of calibration for multiple classes, subsuming the previous two definitions.\nA probabilistic classifier \\mathbf{\\hat{p}}:\\mathcal{X}\\to\\Delta_{k} is multiclass-calibrated if for any prediction vector \\mathbf{q}=(q_1,\\dots,q_k)\\in\\Delta_{k}, the proportions of classes among all possible instances \\mathbf{x} getting the same prediction \\mathbf{\\hat{p}}(\\mathbf{x})=\\mathbf{q} are equal to the prediction vector \\mathbf{q}:\n\\begin{align*} %\\label{eq:calib}\nP(Y=i\\mid \\mathbf{\\hat{p}}(\\mathbf{x})=\\mathbf{q})=q_i\\qquad\\text{for }\\ i=1,\\dots,k.\n\\end{align*}\n\n\nReminder: binning needed\nFor practical purposes, the conditions in these definitions need to be relaxed. This is where binning comes in.\nOnce we have the bins, we can draw a reliability diagram as in the two-class case. For class-wise calibration, we can show per-class reliability diagrams or a single averaged one.\nThe degree of calibration is assessed using the gaps in the reliability diagram. All of this will be elaborated in the next part of the tutorial.\n\n\nImportant points to remember\n\nOnly well-calibrated probability estimates are worthy to be called probabilities: otherwise they are just scores that happen to be in the [0,1] range.\nBinning will be required in some form: instance-based probability evaluation metrics such as Brier score or log-loss always measure calibration plus something else.\nIn multi-class settings, think carefully about which form of calibration you need: e.g., confidence-calibration is too weak in a cost-sensitive setting.\n\n\n\nQuestions and answers\n\n\n\n\n\n\nModel scores:\n\n\n\nCan we interpret the output of any model that produces values between zero and one as probabilities?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect. Some models have been trained to generate values in any arbitrary range, but this does not mean that the model is predicting actual probabilities.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIs there only one way to measure calibration for multiclass probability scores.\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect. There are multiple measures of calibration.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan we perform optimal decisions in a multiclass setting by knowing the highest probability among the classes and the misclassification costs?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect. We need the probabilities of every class in order to make an optimal decision.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nIf the data distribution, operating conditions and the missclassification costs do not change from training to test set, and a model makes optimal predictions in the training set. Do we need the exact probabilities in the test set to make optimal decisions?\n\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nIncorrect. Try another answer.\n\n\n\n\n\n\n\n\n\nAnswer: No.\n\n\n\n\n\nCorrect.",
    "crumbs": [
      "Classifier Calibration"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "This website is authored in Quarto. See New Ways of Publishing: A Roadmap to Authoring Online Training Material for a detailed description of how this training material was created and the formats and tools used.\n\n\n\nThis website contains two short courses related to Trustworthy AI. The content can be viewed in a variety of ways.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#foundations-of-trustworthy-ai",
    "href": "index.html#foundations-of-trustworthy-ai",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "This website is authored in Quarto. See New Ways of Publishing: A Roadmap to Authoring Online Training Material for a detailed description of how this training material was created and the formats and tools used.\n\n\n\nThis website contains two short courses related to Trustworthy AI. The content can be viewed in a variety of ways.",
    "crumbs": [
      "Home"
    ]
  }
]