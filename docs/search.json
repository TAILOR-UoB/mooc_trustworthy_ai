[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Trustworthy AI",
    "section": "",
    "text": "This topic covers the dimensions of Trustworthy AI: (i) Explainability, (ii) Safety, (iii) Fairness, (iv) Accountability and Reproducibility, (v) Privacy, and (vi) Sustainability."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Foundations of Trustworthy AI",
    "section": "About",
    "text": "About\nThis MOOC is based on the first topic proposed in the TAILOR PhD curriculum about Integrating Approaches for Trustworthy AI. The idea is to adapt the content of The TAILOR Handbook of Trustworthy AI and create a course with six subtopics\n\nExplainable AI Systems\nSafety and Robustness\nFairness, Equity, and Justice by Design\nAccountability and Reproducibility\nRespect for Privacy\nSustainability\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#37BEED',\n      'primaryTextColor': '#28306C',\n      'primaryBorderColor': '#28306C',\n      'lineColor': '#37BEED',\n      'secondaryColor': '#37BEED',\n      'tertiaryColor': '#fff'\n    }\n  }\n}%%\nflowchart TD\n  A[Explainable <br> AI Systems] --> B(Accountability <br> and Reproducibility)\n  A --> C(Safety and <br> Robustness)\n  B --> D[Sustainability]\n  B --> E[Respect for Privacy]\n  B --> F[Fairness, Equity, <br> and Justice <br> by Design]\n  C --> F"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Foundations of Trustworthy AI",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to …"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a placeholder for an introduction."
  },
  {
    "objectID": "cha_wahcc/wahcc.html#it-always-starts-with-the-weather",
    "href": "cha_wahcc/wahcc.html#it-always-starts-with-the-weather",
    "title": "5  Why and How Classifier Calibration?",
    "section": "5.1 It always starts with the weather",
    "text": "5.1 It always starts with the weather\n\n5.1.1 Taking inspiration from forecasting\n\nWeather forecasters started thinking about calibration a long time ago (Brier 1950).\n\nA forecast 70% chance of rain should be followed by rain 70% of the time.\n\nThis is immediately applicable to binary classification:\n\nA prediction 70% chance of spam should be spam 70% of the time.\n\nand to multi-class classification:\n\nA prediction 70% chance of setosa, 10% chance of versicolor and 20% chance of virginica should be setosa/versicolor/virginica 70/10/20% of the time.\n\nIn general:\n\nA predicted probability (vector) should match empirical (observed) probabilities.\n\n\nQ: What does X% of the time mean?\n\n\n5.1.2 Forecasting example\nLet’s consider a small toy example:\n\nTwo predictions of 10% chance of rain were both followed by no rain.\nTwo predictions of 40% chance of rain were once followed by no rain, and once by rain.\nThree predictions of 70% chance of rain were once followed by no rain, and twice by rain.\nOne prediction of 90% chance of rain was followed by rain.\n\nQ: Is this forecaster well-calibrated?\n\n\n5.1.3 Over- and under-estimates\n\nTable with probabilities\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n0.1\n0.1\n0\n0\n\n\n2\n3\n0.4\n0.4\n0\n1\n\n\n4\n5\n6\n0.7\n0.7\n0.7\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\nThis forecaster is doing a pretty decent job:\n\n10% chance of rain was a slight over-estimate (\\(\\bar{y} = 0/2 = 0\\%\\));\n40% chance of rain was a slight under-estimate (\\(\\bar{y} = 1/2 = 50\\%\\));\n70% chance of rain was a slight over-estimate (\\(\\bar{y} = 2/3 = 67\\%\\));\n90% chance of rain was a slight under-estimate (\\(\\bar{y} = 1/1 = 100\\%\\)).\n\n\n\n5.1.4 Visualising forecasts: the reliability diagram\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.1 ,0.4, 0.4,0.7, 0.7, 0.7, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\nFigure 5.1: Reliability diagram\n\n\n\n\n\n\n5.1.5 Changing the numbers slightly\n\nTable with probabilities\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n0.1\n0.2\n0\n0\n\n\n2\n3\n0.3\n0.4\n0\n1\n\n\n4\n5\n6\n0.6\n0.7\n0.8\n0\n1\n1\n\n\n7\n0.9\n1\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.25, 0.5, 0.85, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\nFigure 5.2: Reliability diagram\n\n\n\n\n\n\n5.1.6 Or should we group the forecasts differently?\n\nTable with probabilities\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n1\n2\n3\n0.1\n0.2\n0.3\n0.4\n0\n0\n0\n1\n\n\n4\n5\n6\n7\n0.6\n0.7\n0.8\n0.9\n0\n1\n1\n1\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.5, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\nFigure 5.3: Reliability diagram\n\n\n\n\n\n\n5.1.7 Or not at all?\n\nTable with probabilities\n\n\n\n\n\n\n\n\n\\(\\hat{p}\\)\n\\(y\\)\n\n\n\n\n0\n0.1\n0\n\n\n1\n0.2\n0\n\n\n2\n0.3\n0\n\n\n3\n0.4\n1\n\n\n4\n0.6\n0\n\n\n5\n0.7\n1\n\n\n6\n0.8\n1\n\n\n7\n0.9\n1\n\n\n\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pycalib.visualisations import plot_reliability_diagram\n\nlabels = np.array([0, 0, 0, 1, 0, 1, 1, 1])\nscores = np.array([0.1, 0.2 ,0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\nbins = [0, 0.101, 0.201, 0.301, 0.401, 0.601, 0.701, 0.801, 0.901, 1.0]\nfig = plt.figure(figsize=(5, 4))\nfig = plot_reliability_diagram(labels, np.vstack([1 - scores, scores]).T,\n                               class_names=['not 1', 'rain'], bins=bins,\n                               fig=fig, show_gaps=True,\n                               show_bars=True)\n\n\n\n\n\nFigure 5.4: Reliability diagram\n\n\n\n\n\n\n5.1.8 Binning or pooling predictions is a fundamental notion\nWe need bins to evaluate the degree of calibration:\n\nIn order to decide whether a weather forecaster is well-calibrated, we need to look at a good number of forecasts, say over one year.\nWe also need to make sure that there are a reasonable number of forecasts for separate probability values, so we can obtain reliable empirical estimates.\n\nTrade-off: large bins give better empirical estimates, small bins allows a more fine-grained assessment of calibration.}\n\n\nBut adjusting forecasts in groups also gives rise to practical calibration methods:\n\nempirical binning\nisotonic regression (aka ROC convex hull)"
  },
  {
    "objectID": "cha_wahcc/wahcc.html#why-are-we-interested-in-calibration",
    "href": "cha_wahcc/wahcc.html#why-are-we-interested-in-calibration",
    "title": "5  Why and How Classifier Calibration?",
    "section": "5.2 Why are we interested in calibration?",
    "text": "5.2 Why are we interested in calibration?\n\n5.2.1 Why are we interested in calibration?\nTo calibrate means to employ a known scale with known properties.\n\nE.g., additive scale with a well-defined zero, so that ratios are meaningful.\n\nFor classifiers we want to use the probability scale, so that we can\n\njustifiably use default decision rules (e.g., maximum posterior probability);\nadjust these decision rules in a straightforward way to account for different class priors or misclassification costs;\ncombine probability estimates in a well-founded way.\n\nQ: Is the probability scale additive?\nQ: How would you combine probability estimates from several well-calibrated models?\n\n\n5.2.2 Optimal decisions\nDenote the cost of predicting class \\(j\\) for an instance of true class \\(i\\) as \\(C(\\hat{Y}=j|Y=i)\\). % Since we don’t know the true class of an unlabelled instance, we need to base our prediction on an assessment of the expected cost over all possible true classes. The expected cost of predicting class \\(j\\) for instance \\(x\\) is\n\\[\n\\begin{align*}%\\label{eq::expected-cost}\nC(\\hat{Y}=j|X=x) &= \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\\end{align*}\n\\]\nwhere \\(P(Y=i|X=x)\\) is the probability of instance \\(x\\) having true class \\(i\\) (as would be given by the Bayes-optimal classifier).\nThe optimal decision is then to predict the class with lowest expected cost:\n\\[\n\\begin{align*}\n\\hat{Y}^* = \\argmin_j C(\\hat{Y}=j|X=x) = \\arg\\min_j \\sum_i P(Y=i|X=x)C(\\hat{Y}=j|Y=i)\n\\end{align*}\n\\] ## References\n\n\n\n\nBrier, Glenn W. 1950. “Verification of forecasts expressed in terms of probabilities.” Monthly Weather Review 78 (1): 1–3."
  }
]